{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet - Génération de scénarios et renforcement - Denis Lemarchand",
      "provenance": [],
      "collapsed_sections": [
        "A26QiMPtfWwz",
        "T6DV3sCmwcvS",
        "8HxGQlpx3-sn"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMZvTcc5geEeIPXtAjFOkNB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delemarchand2020/DeepLearning/blob/main/Projet_G%C3%A9n%C3%A9ration_de_sc%C3%A9narios_et_renforcement_Denis_Lemarchand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation des librairies"
      ],
      "metadata": {
        "id": "1tppUeEAifNa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YLg5zBhh9U1"
      },
      "outputs": [],
      "source": [
        "!pip install chess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockfish"
      ],
      "metadata": {
        "id": "FTZQo2ZTCNFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/sh/75gzfgu7qo94pvh/AACk_w5M94GTwwhSItCqsemoa/Stockfish%205/stockfish-5-linux.zip\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1gnoo7zvmhn35gUy093Eltw6a-PWOIpwz' -O stockfish-5-linux.zip"
      ],
      "metadata": {
        "id": "nzjINu9FFT-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip stockfish-5-linux.zip"
      ],
      "metadata": {
        "id": "vASiby5QhA5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x stockfish-5-linux/Linux/stockfish_14053109_x64"
      ],
      "metadata": {
        "id": "SzmK4lPdF6vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l stockfish-5-linux/Linux/stockfish_14053109_x64"
      ],
      "metadata": {
        "id": "FaMdxwakGiHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tests python-chess"
      ],
      "metadata": {
        "id": "SOkzLx8ViyO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "max_iter = 1000\n",
        "k = 0\n",
        "\n",
        "board = chess.Board()\n",
        "while not board.is_game_over() and k < max_iter:\n",
        "    if(board.turn): #blanc\n",
        "      white_player = engine.play(board, chess.engine.Limit(time=0.01, depth=2, nodes=5))\n",
        "      board.push(white_player.move)\n",
        "    else:\n",
        "      black_player = engine.play(board, chess.engine.Limit(time=0.01))\n",
        "      board.push(black_player.move)\n",
        "    k += 1\n",
        "\n",
        "engine.quit()\n",
        "\n",
        "print(board.status, k, board.result())\n",
        "board"
      ],
      "metadata": {
        "id": "ITbZFvDXGbhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fonctions utilitaires (issues de RLC)"
      ],
      "metadata": {
        "id": "8HxGQlpx3-sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_project_legal_moves(board):\n",
        "        \"\"\"\n",
        "        Create a mask of legal actions\n",
        "        Returns: np.ndarray with shape (64,64)\n",
        "        \"\"\"\n",
        "        action_space = np.zeros(shape=(64, 64))\n",
        "        moves = [[x.from_square, x.to_square] for x in board.generate_legal_moves()]\n",
        "        for move in moves:\n",
        "            action_space[move[0], move[1]] = 1\n",
        "        return action_space"
      ],
      "metadata": {
        "id": "BROUmwy3BSfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layer_board(board):\n",
        "        \"\"\"\n",
        "        Initalize the numerical representation of the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        for i in range(64):\n",
        "            row = i // 8\n",
        "            col = i % 8\n",
        "            piece = board.piece_at(i)\n",
        "            if piece == None:\n",
        "                continue\n",
        "            elif piece.symbol().isupper():\n",
        "                sign = 1\n",
        "            else:\n",
        "                sign = -1\n",
        "            layer = mapper[piece.symbol()]\n",
        "            layer_board[layer, row, col] = sign\n",
        "        if board.turn:\n",
        "            layer_board[6, :, :] = 1 / board.fullmove_number\n",
        "        if board.can_claim_draw():\n",
        "            layer_board[7, :, :] = 1\n",
        "        return layer_board"
      ],
      "metadata": {
        "id": "aqGIOzYMAvNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_white_move(agent, board, debug=False, best_probs=False):        \n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  action_spaces = []\n",
        "\n",
        "  state = get_layer_board(board)\n",
        "  action_space = get_project_legal_moves(board) # The environment determines which moves are legal\n",
        "  action_probs = agent.model.predict([np.expand_dims(state, axis=0),\n",
        "                                       np.zeros((1, 1)),\n",
        "                                       action_space.reshape(1, 4096)])\n",
        "  action_probs = action_probs / action_probs.sum()\n",
        "  move = np.random.choice(range(4096), p=np.squeeze(action_probs))\n",
        "  if(debug):\n",
        "    print(move, np.argmax(action_probs, axis=1)[0])\n",
        "  if(best_probs):\n",
        "    move = np.argmax(action_probs, axis=1)[0]\n",
        "  move_from = move // 64\n",
        "  move_to = move % 64\n",
        "  moves = [x for x in board.generate_legal_moves() if \\\n",
        "           x.from_square == move_from and x.to_square == move_to]\n",
        "  assert len(moves) > 0  # should not be possible\n",
        "  if len(moves) > 1:\n",
        "    move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "  elif len(moves) == 1:\n",
        "    move = moves[0]\n",
        "  return move      "
      ],
      "metadata": {
        "id": "DjWPk8Ps-zrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_action(board):\n",
        "  legal_moves = [x for x in board.generate_legal_moves()]\n",
        "  legal_moves = np.random.choice(legal_moves)\n",
        "  return legal_moves"
      ],
      "metadata": {
        "id": "HNPwBtHL_fHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RLC \"capture\"\n",
        "[Reprise du code de la librairie RLC de arjangroen](https://github.com/arjangroen/RLC/tree/master/RLC/capture_chess)"
      ],
      "metadata": {
        "id": "A26QiMPtfWwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agent\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras.models import Model, clone_model\n",
        "from keras.layers import Input, Conv2D, Dense, Reshape, Dot, Activation, Multiply\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def policy_gradient_loss(Returns):\n",
        "    def modified_crossentropy(action, action_probs):\n",
        "        cost = (K.categorical_crossentropy(action, action_probs, from_logits=False, axis=1) * Returns)\n",
        "        return K.mean(cost)\n",
        "\n",
        "    return modified_crossentropy\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, gamma=0.5, network='linear', lr=0.01, verbose=0):\n",
        "        \"\"\"\n",
        "        Agent that plays the white pieces in capture chess\n",
        "        Args:\n",
        "            gamma: float\n",
        "                Temporal discount factor\n",
        "            network: str\n",
        "                'linear' or 'conv'\n",
        "            lr: float\n",
        "                Learning rate, ideally around 0.1\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "        self.network = network\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose\n",
        "        self.init_network()\n",
        "        self.weight_memory = []\n",
        "        self.long_term_mean = []\n",
        "\n",
        "    def init_network(self):\n",
        "        \"\"\"\n",
        "        Initialize the network\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if self.network == 'linear':\n",
        "            self.init_linear_network()\n",
        "        elif self.network == 'conv':\n",
        "            self.init_conv_network()\n",
        "        elif self.network == 'conv_pg':\n",
        "            self.init_conv_pg()\n",
        "\n",
        "    def fix_model(self):\n",
        "        \"\"\"\n",
        "        The fixed model is the model used for bootstrapping\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        self.fixed_model = clone_model(self.model)\n",
        "        self.fixed_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "        self.fixed_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def init_linear_network(self):\n",
        "        \"\"\"\n",
        "        Initialize a linear neural network\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n",
        "        reshape_input = Reshape((512,))(input_layer)\n",
        "        output_layer = Dense(4096)(reshape_input)\n",
        "        self.model = Model(inputs=[input_layer], outputs=[output_layer])\n",
        "        self.model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    def init_conv_network(self):\n",
        "        \"\"\"\n",
        "        Initialize a convolutional neural network\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n",
        "        inter_layer_1 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        inter_layer_2 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        flat_1 = Reshape(target_shape=(1, 64))(inter_layer_1)\n",
        "        flat_2 = Reshape(target_shape=(1, 64))(inter_layer_2)\n",
        "        output_dot_layer = Dot(axes=1)([flat_1, flat_2])\n",
        "        output_layer = Reshape(target_shape=(4096,))(output_dot_layer)\n",
        "        self.model = Model(inputs=[input_layer], outputs=[output_layer])\n",
        "        self.model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    def init_conv_pg(self):\n",
        "        \"\"\"\n",
        "        Convnet net for policy gradients\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n",
        "        R = Input(shape=(1,), name='Rewards')\n",
        "        legal_moves = Input(shape=(4096,), name='legal_move_mask')\n",
        "        inter_layer_1 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        inter_layer_2 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        flat_1 = Reshape(target_shape=(1, 64))(inter_layer_1)\n",
        "        flat_2 = Reshape(target_shape=(1, 64))(inter_layer_2)\n",
        "        output_dot_layer = Dot(axes=1)([flat_1, flat_2])\n",
        "        output_layer = Reshape(target_shape=(4096,))(output_dot_layer)\n",
        "        softmax_layer = Activation('softmax')(output_layer)\n",
        "        legal_softmax_layer = Multiply()([legal_moves, softmax_layer])  # Select legal moves\n",
        "        self.model = Model(inputs=[input_layer, R, legal_moves], outputs=[legal_softmax_layer])\n",
        "        self.model.compile(optimizer=optimizer, loss=policy_gradient_loss(R))\n",
        "\n",
        "    def network_update(self, minibatch):\n",
        "        \"\"\"\n",
        "        Update the Q-network using samples from the minibatch\n",
        "        Args:\n",
        "            minibatch: list\n",
        "                The minibatch contains the states, moves, rewards and new states.\n",
        "\n",
        "        Returns:\n",
        "            td_errors: np.array\n",
        "                array of temporal difference errors\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Prepare separate lists\n",
        "        states, moves, rewards, new_states = [], [], [], []\n",
        "        td_errors = []\n",
        "        episode_ends = []\n",
        "        for sample in minibatch:\n",
        "            states.append(sample[0])\n",
        "            moves.append(sample[1])\n",
        "            rewards.append(sample[2])\n",
        "            new_states.append(sample[3])\n",
        "\n",
        "            # Episode end detection\n",
        "            if np.array_equal(sample[3], sample[3] * 0):\n",
        "                episode_ends.append(0)\n",
        "            else:\n",
        "                episode_ends.append(1)\n",
        "\n",
        "        # The Q target\n",
        "        q_target = np.array(rewards) + np.array(episode_ends) * self.gamma * np.max(\n",
        "            self.fixed_model.predict(np.stack(new_states, axis=0)), axis=1)\n",
        "\n",
        "        # The Q value for the remaining actions\n",
        "        q_state = self.model.predict(np.stack(states, axis=0))  # batch x 64 x 64\n",
        "\n",
        "        # Combine the Q target with the other Q values.\n",
        "        q_state = np.reshape(q_state, (len(minibatch), 64, 64))\n",
        "        for idx, move in enumerate(moves):\n",
        "            td_errors.append(q_state[idx, move[0], move[1]] - q_target[idx])\n",
        "            q_state[idx, move[0], move[1]] = q_target[idx]\n",
        "        q_state = np.reshape(q_state, (len(minibatch), 4096))\n",
        "\n",
        "        # Perform a step of minibatch Gradient Descent.\n",
        "        self.model.fit(x=np.stack(states, axis=0), y=q_state, epochs=1, verbose=0)\n",
        "\n",
        "        return td_errors\n",
        "\n",
        "    def get_action_values(self, state):\n",
        "        \"\"\"\n",
        "        Get action values of a state\n",
        "        Args:\n",
        "            state: np.ndarray with shape (8,8,8)\n",
        "                layer_board representation\n",
        "\n",
        "        Returns:\n",
        "            action values\n",
        "\n",
        "        \"\"\"\n",
        "        return self.fixed_model.predict(state) + np.random.randn() * 1e-9\n",
        "\n",
        "    def policy_gradient_update(self, states, actions, rewards, action_spaces, actor_critic=False):\n",
        "        \"\"\"\n",
        "        Update parameters with Monte Carlo Policy Gradient algorithm\n",
        "        Args:\n",
        "            states: (list of tuples) state sequence in episode\n",
        "            actions: action sequence in episode\n",
        "            rewards: rewards sequence in episode\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        n_steps = len(states)\n",
        "        Returns = []\n",
        "        targets = np.zeros((n_steps, 64, 64))\n",
        "        for t in range(n_steps):\n",
        "            action = actions[t]\n",
        "            targets[t, action[0], action[1]] = 1\n",
        "            if actor_critic:\n",
        "                R = rewards[t, action[0] * 64 + action[1]]\n",
        "            else:\n",
        "                R = np.sum([r * self.gamma ** i for i, r in enumerate(rewards[t:])])\n",
        "            Returns.append(R)\n",
        "\n",
        "        if not actor_critic:\n",
        "            mean_return = np.mean(Returns)\n",
        "            self.long_term_mean.append(mean_return)\n",
        "            train_returns = np.stack(Returns, axis=0) - np.mean(self.long_term_mean)\n",
        "        else:\n",
        "            train_returns = np.stack(Returns, axis=0)\n",
        "        # print(train_returns.shape)\n",
        "        targets = targets.reshape((n_steps, 4096))\n",
        "        self.weight_memory.append(self.model.get_weights())\n",
        "        self.model.fit(x=[np.stack(states, axis=0),\n",
        "                          train_returns,\n",
        "                          np.concatenate(action_spaces, axis=0)\n",
        "                          ],\n",
        "                       y=[np.stack(targets, axis=0)],\n",
        "                       verbose=self.verbose\n",
        "                       )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OiX0CWLhfbNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Environnement\n",
        "import chess\n",
        "import chess.engine\n",
        "import numpy as np\n",
        "\n",
        "mapper = {}\n",
        "mapper[\"p\"] = 0\n",
        "mapper[\"r\"] = 1\n",
        "mapper[\"n\"] = 2\n",
        "mapper[\"b\"] = 3\n",
        "mapper[\"q\"] = 4\n",
        "mapper[\"k\"] = 5\n",
        "mapper[\"P\"] = 0\n",
        "mapper[\"R\"] = 1\n",
        "mapper[\"N\"] = 2\n",
        "mapper[\"B\"] = 3\n",
        "mapper[\"Q\"] = 4\n",
        "mapper[\"K\"] = 5\n",
        "\n",
        "\n",
        "class Board(object):\n",
        "\n",
        "    def __init__(self, FEN=None):\n",
        "        \"\"\"\n",
        "        Chess Board Environment\n",
        "        Args:\n",
        "            FEN: str\n",
        "                Starting FEN notation, if None then start in the default chess position\n",
        "        \"\"\"\n",
        "        self.FEN = FEN\n",
        "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "        self.init_action_space()\n",
        "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        self.init_layer_board()\n",
        "\n",
        "    def init_action_space(self):\n",
        "        \"\"\"\n",
        "        Initialize the action space\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        self.action_space = np.zeros(shape=(64, 64))\n",
        "\n",
        "    def init_layer_board(self):\n",
        "        \"\"\"\n",
        "        Initalize the numerical representation of the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        for i in range(64):\n",
        "            row = i // 8\n",
        "            col = i % 8\n",
        "            piece = self.board.piece_at(i)\n",
        "            if piece == None:\n",
        "                continue\n",
        "            elif piece.symbol().isupper():\n",
        "                sign = 1\n",
        "            else:\n",
        "                sign = -1\n",
        "            layer = mapper[piece.symbol()]\n",
        "            self.layer_board[layer, row, col] = sign\n",
        "        if self.board.turn:\n",
        "            self.layer_board[6, :, :] = 1 / self.board.fullmove_number\n",
        "        if self.board.can_claim_draw():\n",
        "            self.layer_board[7, :, :] = 1\n",
        "\n",
        "    def step(self, action, opponent=None):\n",
        "        \"\"\"\n",
        "        Run a step\n",
        "        Args:\n",
        "            action: tuple of 2 integers\n",
        "                Move from, Move to\n",
        "        Returns:\n",
        "            epsiode end: Boolean\n",
        "                Whether the episode has ended\n",
        "            reward: int\n",
        "                Difference in material value after the move\n",
        "        \"\"\"\n",
        "        piece_balance_before = self.get_material_value()\n",
        "        self.board.push(action)\n",
        "        self.init_layer_board()\n",
        "        piece_balance_after = self.get_material_value()\n",
        "        if self.board.result() == \"*\":\n",
        "            if opponent == None:\n",
        "              opponent_move = self.get_random_action()\n",
        "            else:\n",
        "              PlayResult = opponent.play(self.board, chess.engine.Limit(time=0.1, depth=4, nodes=4))\n",
        "              opponent_move = PlayResult.move\n",
        "            self.board.push(opponent_move)\n",
        "            self.init_layer_board() \n",
        "            capture_reward = piece_balance_after - piece_balance_before\n",
        "            if self.board.result() == \"*\":\n",
        "                reward = 0 + capture_reward\n",
        "                episode_end = False\n",
        "            else:\n",
        "                if self.board.result() == \"1-0\":\n",
        "                  reward = 40 + capture_reward\n",
        "                else:\n",
        "                  reward = 0 + capture_reward\n",
        "                episode_end = True\n",
        "        else:\n",
        "            capture_reward = piece_balance_after - piece_balance_before\n",
        "            if self.board.result() == \"1-0\":\n",
        "                  reward = 40 + capture_reward\n",
        "            else:\n",
        "                  reward = 0 + capture_reward\n",
        "            episode_end = True\n",
        "        if self.board.is_game_over():\n",
        "            reward = 0\n",
        "            episode_end = True\n",
        "        return episode_end, reward\n",
        "\n",
        "    def get_random_action(self):\n",
        "        \"\"\"\n",
        "        Sample a random action\n",
        "        Returns: move\n",
        "            A legal python chess move.\n",
        "        \"\"\"\n",
        "        legal_moves = [x for x in self.board.generate_legal_moves()]\n",
        "        legal_moves = np.random.choice(legal_moves)\n",
        "        return legal_moves\n",
        "\n",
        "    def project_legal_moves(self):\n",
        "        \"\"\"\n",
        "        Create a mask of legal actions\n",
        "        Returns: np.ndarray with shape (64,64)\n",
        "        \"\"\"\n",
        "        self.action_space = np.zeros(shape=(64, 64))\n",
        "        moves = [[x.from_square, x.to_square] for x in self.board.generate_legal_moves()]\n",
        "        for move in moves:\n",
        "            self.action_space[move[0], move[1]] = 1\n",
        "        return self.action_space\n",
        "\n",
        "    def get_material_value(self):\n",
        "        \"\"\"\n",
        "        Sums up the material balance using Reinfield values\n",
        "        Returns: The material balance on the board\n",
        "        \"\"\"\n",
        "        pawns = 1 * np.sum(self.layer_board[0, :, :])\n",
        "        rooks = 5 * np.sum(self.layer_board[1, :, :])\n",
        "        minor = 3 * np.sum(self.layer_board[2:4, :, :])\n",
        "        queen = 9 * np.sum(self.layer_board[4, :, :])\n",
        "        return pawns + rooks + minor + queen\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "        self.init_layer_board()\n",
        "        self.init_action_space()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Bs6WX8iyi91z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Learn\n",
        "import numpy as np\n",
        "from chess.pgn import Game\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Q_learning(object):\n",
        "\n",
        "    def __init__(self, agent, env, memsize=1000):\n",
        "        \"\"\"\n",
        "        Reinforce object to learn capture chess\n",
        "        Args:\n",
        "            agent: The agent playing the chess game as white\n",
        "            env: The environment including the python-chess board\n",
        "            memsize: maximum amount of games to retain in-memory\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.memory = []\n",
        "        self.memsize = memsize\n",
        "        self.reward_trace = []\n",
        "        self.memory = []\n",
        "        self.sampling_probs = []\n",
        "\n",
        "    def learn(self, iters=100, c=10, windows_r_average=10):\n",
        "        \"\"\"\n",
        "        Run the Q-learning algorithm. Play greedy on the final iter\n",
        "        Args:\n",
        "            iters: int\n",
        "                amount of games to train\n",
        "            c: int\n",
        "                update the network every c games\n",
        "\n",
        "        Returns: pgn (str)\n",
        "            pgn string describing final game\n",
        "\n",
        "        \"\"\"\n",
        "        for k in range(iters):\n",
        "            greedy = True if k == iters - 1 else False\n",
        "            self.env.reset()\n",
        "            self.play_game(k, greedy=greedy)\n",
        "            if k % c == 0:\n",
        "                all_r = np.array(self.reward_trace)\n",
        "                r_average = all_r[-windows_r_average:].mean()\n",
        "                print(\"iter\", k, \" r_average\", r_average)\n",
        "                self.agent.fix_model()\n",
        "\n",
        "        pgn = Game.from_board(self.env.board)\n",
        "        reward_smooth = pd.DataFrame(self.reward_trace)\n",
        "        reward_smooth.rolling(window=10, min_periods=0).mean().plot()\n",
        "\n",
        "        return pgn\n",
        "\n",
        "    def play_game(self, k, greedy=False, maxiter=25):\n",
        "        \"\"\"\n",
        "        Play a game of capture chess\n",
        "        Args:\n",
        "            k: int\n",
        "                game count, determines epsilon (exploration rate)\n",
        "            greedy: Boolean\n",
        "                if greedy, no exploration is done\n",
        "            maxiter: int\n",
        "                Maximum amount of steps per game\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "\n",
        "        # Here we determine the exploration rate. k is divided by 250 to slow down the exploration rate decay.\n",
        "        eps = max(0.05, 1 / (1 + (k / 250))) if not greedy else 0.\n",
        "\n",
        "        # Play a game of chess\n",
        "        while not episode_end:\n",
        "            state = self.env.layer_board\n",
        "            explore = np.random.uniform(0, 1) < eps  # determine whether to explore\n",
        "            if explore:\n",
        "                move = self.env.get_random_action()\n",
        "                move_from = move.from_square\n",
        "                move_to = move.to_square\n",
        "            else:\n",
        "                action_values = self.agent.get_action_values(np.expand_dims(state, axis=0))\n",
        "                action_values = np.reshape(np.squeeze(action_values), (64, 64))\n",
        "                action_space = self.env.project_legal_moves()  # The environment determines which moves are legal\n",
        "                action_values = np.multiply(action_values, action_space)\n",
        "                move_from = np.argmax(action_values, axis=None) // 64\n",
        "                move_to = np.argmax(action_values, axis=None) % 64\n",
        "                moves = [x for x in self.env.board.generate_legal_moves() if \\\n",
        "                         x.from_square == move_from and x.to_square == move_to]\n",
        "                if len(moves) == 0:  # If all legal moves have negative action value, explore.\n",
        "                    move = self.env.get_random_action()\n",
        "                    move_from = move.from_square\n",
        "                    move_to = move.to_square\n",
        "                else:\n",
        "                    move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "\n",
        "            episode_end, reward = self.env.step(move)\n",
        "            new_state = self.env.layer_board\n",
        "            if len(self.memory) > self.memsize:\n",
        "                self.memory.pop(0)\n",
        "                self.sampling_probs.pop(0)\n",
        "            turncount += 1\n",
        "            if turncount > maxiter:\n",
        "                episode_end = True\n",
        "                reward = 0\n",
        "            if episode_end:\n",
        "                new_state = new_state * 0\n",
        "            self.memory.append([state, (move_from, move_to), reward, new_state])\n",
        "            self.sampling_probs.append(1)\n",
        "\n",
        "            self.reward_trace.append(reward)\n",
        "\n",
        "            self.update_agent(turncount)\n",
        "\n",
        "        return self.env.board\n",
        "\n",
        "    def sample_memory(self, turncount):\n",
        "        \"\"\"\n",
        "        Get a sample from memory for experience replay\n",
        "        Args:\n",
        "            turncount: int\n",
        "                turncount limits the size of the minibatch\n",
        "\n",
        "        Returns: tuple\n",
        "            a mini-batch of experiences (list)\n",
        "            indices of chosen experiences\n",
        "\n",
        "        \"\"\"\n",
        "        minibatch = []\n",
        "        memory = self.memory[:-turncount]\n",
        "        probs = self.sampling_probs[:-turncount]\n",
        "        sample_probs = [probs[n] / np.sum(probs) for n in range(len(probs))]\n",
        "        indices = np.random.choice(range(len(memory)), min(1028, len(memory)), replace=True, p=sample_probs)\n",
        "        for i in indices:\n",
        "            minibatch.append(memory[i])\n",
        "\n",
        "        return minibatch, indices\n",
        "\n",
        "    def update_agent(self, turncount):\n",
        "        \"\"\"\n",
        "        Update the agent using experience replay. Set the sampling probs with the td error\n",
        "        Args:\n",
        "            turncount: int\n",
        "                Amount of turns played. Only sample the memory of there are sufficient samples\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if turncount < len(self.memory):\n",
        "            minibatch, indices = self.sample_memory(turncount)\n",
        "            td_errors = self.agent.network_update(minibatch)\n",
        "            for n, i in enumerate(indices):\n",
        "                self.sampling_probs[i] = np.abs(td_errors[n])\n",
        "\n",
        "\n",
        "class Reinforce(object):\n",
        "\n",
        "    def __init__(self, agent, env, opponent=None):\n",
        "        \"\"\"\n",
        "        Reinforce object to learn capture chess\n",
        "        Args:\n",
        "            agent: The agent playing the chess game as white\n",
        "            env: The environment including the python-chess board\n",
        "            memsize: maximum amount of games to retain in-memory\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.reward_trace = []\n",
        "        self.action_value_mem = []\n",
        "        self.opponent = opponent\n",
        "\n",
        "    def learn(self, iters=100, c=10):\n",
        "        \"\"\"\n",
        "        Run the Q-learning algorithm. Play greedy on the final iter\n",
        "        Args:\n",
        "            iters: int\n",
        "                amount of games to train\n",
        "            c: int\n",
        "                update the network every c games\n",
        "\n",
        "        Returns: pgn (str)\n",
        "            pgn string describing final game\n",
        "\n",
        "        \"\"\"\n",
        "        for k in range(iters):\n",
        "            self.env.reset()\n",
        "            states, actions, rewards, action_spaces = self.play_game(k)\n",
        "            self.reinforce_agent(states, actions, rewards, action_spaces)\n",
        "\n",
        "        pgn = Game.from_board(self.env.board)\n",
        "        reward_smooth = pd.DataFrame(self.reward_trace)\n",
        "        reward_smooth.rolling(window=10, min_periods=0).mean().plot()\n",
        "\n",
        "        return pgn\n",
        "\n",
        "    def play_game(self, k, maxiter=25):\n",
        "        \"\"\"\n",
        "        Play a game of capture chess\n",
        "        Args:\n",
        "            k: int\n",
        "                game count, determines epsilon (exploration rate)\n",
        "            greedy: Boolean\n",
        "                if greedy, no exploration is done\n",
        "            maxiter: int\n",
        "                Maximum amount of steps per game\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        action_spaces = []\n",
        "\n",
        "        # Play a game of chess\n",
        "        while not episode_end:\n",
        "            state = self.env.layer_board\n",
        "            action_space = self.env.project_legal_moves()  # The environment determines which moves are legal\n",
        "            action_probs = self.agent.model.predict([np.expand_dims(state, axis=0),\n",
        "                                                     np.zeros((1, 1)),\n",
        "                                                     action_space.reshape(1, 4096)])\n",
        "            self.action_value_mem.append(action_probs)\n",
        "            action_probs = action_probs / action_probs.sum()\n",
        "            move = np.random.choice(range(4096), p=np.squeeze(action_probs))\n",
        "            move_from = move // 64\n",
        "            move_to = move % 64\n",
        "            moves = [x for x in self.env.board.generate_legal_moves() if \\\n",
        "                     x.from_square == move_from and x.to_square == move_to]\n",
        "            assert len(moves) > 0  # should not be possible\n",
        "            if len(moves) > 1:\n",
        "                move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "            elif len(moves) == 1:\n",
        "                move = moves[0]\n",
        "\n",
        "            episode_end, reward = self.env.step(move, self.opponent)\n",
        "            new_state = self.env.layer_board\n",
        "            turncount += 1\n",
        "            if turncount > maxiter:\n",
        "                episode_end = True\n",
        "                reward = 0\n",
        "            if episode_end:\n",
        "                new_state = new_state * 0\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append((move_from, move_to))\n",
        "            rewards.append(reward)\n",
        "            action_spaces.append(action_space.reshape(1, 4096))\n",
        "\n",
        "        self.reward_trace.append(np.sum(rewards))\n",
        "\n",
        "        return states, actions, rewards, action_spaces\n",
        "\n",
        "    def reinforce_agent(self, states, actions, rewards, action_spaces):\n",
        "        \"\"\"\n",
        "        Update the agent using experience replay. Set the sampling probs with the td error\n",
        "        Args:\n",
        "            turncount: int\n",
        "                Amount of turns played. Only sample the memory of there are sufficient samples\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        self.agent.policy_gradient_update(states, actions, rewards, action_spaces)\n",
        "\n",
        "\n",
        "class ActorCritic(object):\n",
        "\n",
        "    def __init__(self, actor, critic, env):\n",
        "        \"\"\"\n",
        "        ActorCritic object to learn capture chess\n",
        "        Args:\n",
        "            actor: Policy Gradient Agent\n",
        "            critic: Q-learning Agent\n",
        "            env: The environment including the python-chess board\n",
        "            memsize: maximum amount of games to retain in-memory\n",
        "        \"\"\"\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.reward_trace = []\n",
        "        self.action_value_mem = []\n",
        "        self.memory = []\n",
        "        self.sampling_probs = []\n",
        "\n",
        "    def learn(self, iters=100, c=10):\n",
        "        \"\"\"\n",
        "        Run the Q-learning algorithm. Play greedy on the final iter\n",
        "        Args:\n",
        "            iters: int\n",
        "                amount of games to train\n",
        "            c: int\n",
        "                update the network every c games\n",
        "\n",
        "        Returns: pgn (str)\n",
        "            pgn string describing final game\n",
        "\n",
        "        \"\"\"\n",
        "        for k in range(iters):\n",
        "            if k % c == 0:\n",
        "                self.critic.fix_model()\n",
        "            self.env.reset()\n",
        "            end_state = self.play_game(k)\n",
        "\n",
        "        pgn = Game.from_board(self.env.board)\n",
        "        reward_smooth = pd.DataFrame(self.reward_trace)\n",
        "        reward_smooth.rolling(window=10, min_periods=0).mean().plot()\n",
        "\n",
        "        return pgn\n",
        "\n",
        "    def play_game(self, k, greedy=False, maxiter=25):\n",
        "        \"\"\"\n",
        "        Play a game of capture chess\n",
        "        Args:\n",
        "            k: int\n",
        "                game count, determines epsilon (exploration rate)\n",
        "            greedy: Boolean\n",
        "                if greedy, no exploration is done\n",
        "            maxiter: int\n",
        "                Maximum amount of steps per game\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "\n",
        "        # Play a game of chess\n",
        "        state = self.env.layer_board\n",
        "        while not episode_end:\n",
        "            state = self.env.layer_board\n",
        "            action_space = self.env.project_legal_moves()  # The environment determines which moves are legal\n",
        "            action_probs = self.actor.model.predict([np.expand_dims(state, axis=0),\n",
        "                                                     np.zeros((1, 1)),\n",
        "                                                     action_space.reshape(1, 4096)])\n",
        "            self.action_value_mem.append(action_probs)\n",
        "            # print(action_probs)\n",
        "            # print(np.max(action_probs))\n",
        "            action_probs = action_probs / action_probs.sum()\n",
        "            move = np.random.choice(range(4096), p=np.squeeze(action_probs))\n",
        "            move_from = move // 64\n",
        "            move_to = move % 64\n",
        "            moves = [x for x in self.env.board.generate_legal_moves() if \\\n",
        "                     x.from_square == move_from and x.to_square == move_to]\n",
        "            assert len(moves) > 0  # should not be possible\n",
        "            if len(moves) > 1:\n",
        "                move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "            elif len(moves) == 1:\n",
        "                move = moves[0]\n",
        "\n",
        "            episode_end, reward = self.env.step(move)\n",
        "            new_state = self.env.layer_board\n",
        "            turncount += 1\n",
        "            if turncount > maxiter:\n",
        "                episode_end = True\n",
        "                reward = 0\n",
        "            if episode_end:\n",
        "                new_state = new_state * 0\n",
        "\n",
        "            self.memory.append([state, (move_from, move_to), reward, new_state, action_space.reshape(1, 4096)])\n",
        "            self.sampling_probs.append(1)\n",
        "            self.reward_trace.append(reward)\n",
        "\n",
        "        self.update_actorcritic(turncount)\n",
        "\n",
        "        return self.env.board\n",
        "\n",
        "    def sample_memory(self, turncount):\n",
        "        \"\"\"\n",
        "        Get a sample from memory for experience replay\n",
        "        Args:\n",
        "            turncount: int\n",
        "                turncount limits the size of the minibatch\n",
        "\n",
        "        Returns: tuple\n",
        "            a mini-batch of experiences (list)\n",
        "            indices of chosen experiences\n",
        "\n",
        "        \"\"\"\n",
        "        minibatch = []\n",
        "        memory = self.memory[:-turncount]\n",
        "        probs = self.sampling_probs[:-turncount]\n",
        "        sample_probs = [probs[n] / np.sum(probs) for n in range(len(probs))]\n",
        "        indices = np.random.choice(range(len(memory)), min(1028, len(memory)), replace=False, p=sample_probs)\n",
        "        for i in indices:\n",
        "            minibatch.append(memory[i])\n",
        "\n",
        "        return minibatch, indices\n",
        "\n",
        "    def update_actorcritic(self, turncount):\n",
        "        \"\"\"Actor critic\"\"\"\n",
        "\n",
        "        if turncount < len(self.memory):\n",
        "\n",
        "            # Get a sampple\n",
        "            minibatch, indices = self.sample_memory(turncount)\n",
        "\n",
        "            # Update critic and find td errors for prioritized experience replay\n",
        "            td_errors = self.critic.network_update(minibatch)\n",
        "\n",
        "            # Get a Q value from the critic\n",
        "            states = [x[0] for x in minibatch]\n",
        "            actions = [x[1] for x in minibatch]\n",
        "            Q_est = self.critic.get_action_values(np.stack(states, axis=0))\n",
        "            action_spaces = [x[4] for x in minibatch]\n",
        "\n",
        "            self.actor.policy_gradient_update(states, actions, Q_est, action_spaces, actor_critic=True)\n",
        "\n",
        "            # Update sampling probs\n",
        "            for n, i in enumerate(indices):\n",
        "                self.sampling_probs[i] = np.abs(td_errors[n])\n",
        "\n",
        "    def update_critic(self, turncount):\n",
        "        \"\"\"\n",
        "        Update the agent using experience replay. Set the sampling probs with the td error\n",
        "        Args:\n",
        "            turncount: int\n",
        "                Amount of turns played. Only sample the memory of there are sufficient samples\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if turncount < len(self.memory):\n",
        "            minibatch, indices = self.sample_memory(turncount)\n",
        "            td_errors = self.critic.network_update(minibatch)\n",
        "\n",
        "            for n, i in enumerate(indices):\n",
        "                self.sampling_probs[i] = np.abs(td_errors[n])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jPfLCxW6jVMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Génération des configurations de l’échiquier avec un $β$-VAE"
      ],
      "metadata": {
        "id": "Tz1xU73NF1c7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Librairies"
      ],
      "metadata": {
        "id": "9AmDigYcuZBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "use_gpu = False\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Exécution sur {device}\")\n",
        "\n",
        "# Imports des bibliothèques utiles\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "8-7hvpJT7N2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparamètres"
      ],
      "metadata": {
        "id": "BNOEUNOavTxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wout = (Win - K + 2P)/S + 1\n",
        "\n",
        "Hout = (Hin - K + 2P)/S + 1\n",
        "\n",
        "K:Kernel\n",
        "\n",
        "P:Padding\n",
        "\n",
        "S:Stride"
      ],
      "metadata": {
        "id": "ES7qL0oq2OtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Kernel_size = 3\n",
        "Stride_size = 1\n",
        "Padding_size = 1\n",
        "N_channels_1 = 12\n",
        "N_channels_2 = 12\n",
        "\n",
        "seuil_proba_prediction = 0.45\n",
        "\n",
        "W_in = 8\n",
        "H_in = 8\n",
        "\n",
        "W_out = (W_in - Kernel_size + 2*Padding_size)/Stride_size + 1\n",
        "H_out = (H_in - Kernel_size + 2*Padding_size)/Stride_size + 1\n",
        "\n",
        "print(W_out,H_out)\n",
        "\n",
        "W_out = int(W_out)\n",
        "H_out = int(H_out)"
      ],
      "metadata": {
        "id": "AnLXeJmW3-Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.3\n",
        "latent_dimension = 100\n",
        "batch_size = 64\n",
        "\n",
        "N_epochs = 80\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "meOZ6RgC2234"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Taille_DA = 80000\n",
        "Taille_DT = 16000"
      ],
      "metadata": {
        "id": "RAhS_BVowFTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conversion échiquier <-> numpy board"
      ],
      "metadata": {
        "id": "PNgGDN1vueVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def board_to_numpy(board):\n",
        "        \"\"\"\n",
        "        Initalize the numerical representation of the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        layer_board = np.zeros(shape=(6, 8, 8))\n",
        "        for i in range(64):\n",
        "            row = i // 8\n",
        "            col = i % 8\n",
        "            piece = board.piece_at(i)\n",
        "            if piece == None:\n",
        "                continue\n",
        "            elif piece.symbol().isupper():\n",
        "                sign = 1\n",
        "            else:\n",
        "                sign = -1\n",
        "            layer = mapper[piece.symbol()]\n",
        "            layer_board[layer, row, col] = sign\n",
        "        return layer_board\n"
      ],
      "metadata": {
        "id": "dfKlO_TMNilN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapper = {}\n",
        "#pieces noires\n",
        "mapper[\"p\"] = 0 #pion \n",
        "mapper[\"r\"] = 1 #tour\n",
        "mapper[\"n\"] = 2 #cavalier\n",
        "mapper[\"b\"] = 3 #fou\n",
        "mapper[\"q\"] = 4 #reine\n",
        "mapper[\"k\"] = 5 #roi\n",
        "#pieces blanches\n",
        "mapper[\"P\"] = 0\n",
        "mapper[\"R\"] = 1\n",
        "mapper[\"N\"] = 2\n",
        "mapper[\"B\"] = 3\n",
        "mapper[\"Q\"] = 4\n",
        "mapper[\"K\"] = 5\n",
        "\n",
        "layer = {}\n",
        "layer[0] = chess.PAWN\n",
        "layer[1] = chess.ROOK\n",
        "layer[2] = chess.KNIGHT\n",
        "layer[3] = chess.BISHOP\n",
        "layer[4] = chess.QUEEN\n",
        "layer[5] = chess.KING\n",
        "\n",
        "def numpy_to_board(layer_board):\n",
        "    board = chess.Board.empty()\n",
        "    for l in range(6):\n",
        "       for row in range(8):\n",
        "          for col in range(8):\n",
        "             piece = layer_board[l][row][col]\n",
        "             if piece == 1: #blanche\n",
        "               c = chess.WHITE\n",
        "             elif piece == -1: #noire\n",
        "               c = chess.BLACK\n",
        "             else:\n",
        "               continue\n",
        "             p = chess.Piece(layer[l],c)\n",
        "             board.set_piece_at(row*8+col, piece=p)\n",
        "    return board      "
      ],
      "metadata": {
        "id": "A7qyS8gLQrtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Préparation du dataset des configurations"
      ],
      "metadata": {
        "id": "OpmEJ2rRulJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurer une instance de stockfish, une zone mémoire D pour stocker les configurations réelles d’échiquiers.\n",
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "class DatasetConfigBoard(object):\n",
        "\n",
        "    def __init__(self, N=1000, N_test=200):\n",
        "        self.N = N\n",
        "        self.memory_DA = torch.Tensor(size=(N,6,8,8))\n",
        "        self.memory_DT = torch.Tensor(size=(N_test,6,8,8))\n",
        "        self.memory_Boards = [] #pour tests\n",
        "        self.N_test = N_test\n",
        "\n",
        "    def generate(self, N, memory_D, memory_Boards):\n",
        "        engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "        i = 0\n",
        "        while i < N:\n",
        "          max_iter = 1000\n",
        "          k = 0\n",
        "          board = chess.Board()\n",
        "          while not board.is_game_over() and k < max_iter and i < N:\n",
        "              if(board.turn): #blanc\n",
        "                white_player = engine.play(board, chess.engine.Limit(time=0.01))\n",
        "                board.push(white_player.move)\n",
        "              else:\n",
        "                black_player = engine.play(board, chess.engine.Limit(time=0.01))\n",
        "                board.push(black_player.move)\n",
        "              k += 1\n",
        "              memory_D[i] = torch.Tensor(board_to_numpy(board))\n",
        "              memory_Boards.append(board.copy())\n",
        "              i += 1\n",
        "        engine.quit()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        self.generate(self.N, self.memory_DA, self.memory_Boards)\n",
        "        self.generate(self.N_test, self.memory_DT, self.memory_Boards)\n",
        "        return self.memory_DA, self.memory_DT"
      ],
      "metadata": {
        "id": "_YW4NaZDGMOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config = DatasetConfigBoard(N=Taille_DA, N_test=Taille_DT)\n",
        "DA, DT = dataset_config.create_dataset()"
      ],
      "metadata": {
        "id": "5Vc5YaFhubcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalisation : -1 -> 0, 0 -> 0.5, 1 -> 1\n",
        "# normalisation(x): (x + 1)/2\n",
        "def normalisation(x):\n",
        "  return (x+1)/2\n",
        "\n",
        "def de_normalisation(x):\n",
        "  return (2*x - 1)"
      ],
      "metadata": {
        "id": "auPkTU5Zjip2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DA_copy = DA\n",
        "DT_copy = DT"
      ],
      "metadata": {
        "id": "eRjaH5MDkv4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DA = normalisation(DA_copy)\n",
        "DT = normalisation(DT_copy)"
      ],
      "metadata": {
        "id": "e3VYbuOuk2on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(DA, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "iBRN8kxwvp1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implémentation $\\beta$-VAE"
      ],
      "metadata": {
        "id": "Oj6gCoiuyYZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dimension):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = nn.Sequential(nn.Conv2d(6, N_channels_1, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Conv2d(N_channels_1, N_channels_2, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Flatten(),\n",
        "                                   )\n",
        "        self.linear1 = nn.Linear(in_features=(N_channels_2*W_out*H_out), out_features=latent_dimension)\n",
        "        self.linear2 = nn.Linear(in_features=(N_channels_2*W_out*H_out), out_features=latent_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x_mu = self.linear1(x)\n",
        "        x_logvar = self.linear2(x)\n",
        "        return x_mu, x_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dimension):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear = nn.Linear(in_features=latent_dimension, out_features=N_channels_2*W_out*H_out)\n",
        "        self.model = nn.Sequential(nn.ConvTranspose2d(N_channels_2, N_channels_1, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.ConvTranspose2d(N_channels_1, 6, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.Sigmoid()\n",
        "                                    )\n",
        "\n",
        "    def forward(self, z):\n",
        "        hat_x = F.relu(self.linear(z))\n",
        "        hat_x = hat_x.view(-1, N_channels_2, W_out, H_out)\n",
        "        hat_x = self.model(hat_x)\n",
        "        return hat_x"
      ],
      "metadata": {
        "id": "Ne0AXC3QyeyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        z = self.latent_sample(latent_mu, latent_logvar)\n",
        "        hat_x = self.decoder(z)\n",
        "        return hat_x, latent_mu, latent_logvar\n",
        "\n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu"
      ],
      "metadata": {
        "id": "rFzM_nDdysVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(hat_x, x, mu, logvar):\n",
        "    reconstruction_loss = F.binary_cross_entropy(hat_x.view(-1, 6*8*8), x.view(-1, 6*8*8), reduction='sum')\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return reconstruction_loss + beta * kl_divergence"
      ],
      "metadata": {
        "id": "h1X9V-KlzIf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apprentissage $\\beta$-VAE"
      ],
      "metadata": {
        "id": "mTeHCTT7zKVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "def train_vae(net, train_dataset, epochs=10, learning_rate=1e-3, batch_size=128, device=device):\n",
        "    # Création du DataLoader pour charger les données\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    # Définition de l'algorithme d'optimisation (Adam, variante de la SGD)\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    # Choix de la fonction de coût\n",
        "    criterion = vae_loss\n",
        "    # Passe le modèle en mode \"apprentissage\"\n",
        "    net = net.to(device)\n",
        "    net = net.train()\n",
        "\n",
        "    t = trange(1, epochs + 1, desc=\"Entraînement du modèle\")\n",
        "    for epoch in t:\n",
        "        avg_loss = 0.\n",
        "        # Parcours du dataset pour une epoch\n",
        "        for images in tqdm(train_dataloader):\n",
        "            # les labels sont ignorés pour l'apprentissage de l'auto-encodeur\n",
        "\n",
        "            images = images.to(device)\n",
        "            # Calcul de la reconstruction\n",
        "            reconstructions, latent_mu, latent_logvar = net(images)\n",
        "            # Calcul de l'erreur\n",
        "            loss = criterion(reconstructions, images, latent_mu, latent_logvar)\n",
        "\n",
        "            # Rétropropagation du gradient\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Descente de gradient (une itération)\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "        avg_loss /= len(train_dataloader)\n",
        "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
        "    return net.to(\"cpu\")"
      ],
      "metadata": {
        "id": "Gx3voumzzPsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VariationalAutoencoder(latent_dimension)\n",
        "train_vae(vae, DA, epochs=N_epochs, learning_rate=learning_rate, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "ja-iuOntzYN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tests de génération de configurations synthétiques"
      ],
      "metadata": {
        "id": "YhTm3l9Ku6oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cas a : la reconstruction est-elle identique à l’entrée, est-elle valide ?"
      ],
      "metadata": {
        "id": "GBKupBu8-qbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_layer_board(fake_config_board, p=0.5):\n",
        "  layer_board = torch.round(fake_config_board, decimals=2)\n",
        "  layer_board[layer_board>p] = 1\n",
        "  layer_board[layer_board<-p] = -1\n",
        "  layer_board[torch.abs(layer_board)!=1] = 0\n",
        "  return layer_board"
      ],
      "metadata": {
        "id": "BeGg0gtV80Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae.eval()\n",
        "reconstruction,_,_ = vae(DT)"
      ],
      "metadata": {
        "id": "FTTwkd-F7kt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_config_board = de_normalisation(reconstruction.detach())\n",
        "reconstruction_layer_board = predict_layer_board(reconstruction_config_board, p=seuil_proba_prediction)"
      ],
      "metadata": {
        "id": "QWHerSLE8KjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 354\n",
        "b = numpy_to_board(reconstruction_layer_board[n].numpy())\n",
        "print(b.status())\n",
        "b"
      ],
      "metadata": {
        "id": "PTX_MVx19U-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config.memory_Boards[len(DA)+n]"
      ],
      "metadata": {
        "id": "cDpmCV9x9uYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(b.board_fen())\n",
        "print(dataset_config.memory_Boards[len(DA)+n].board_fen())"
      ],
      "metadata": {
        "id": "vvKFApLz__yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cas_a_invalide = 0\n",
        "cas_a_invalide_DT = 0\n",
        "cas_a_different = 0\n",
        "\n",
        "invalide_board_DT = []\n",
        "\n",
        "for i in range(len(reconstruction_layer_board)):\n",
        "  b = numpy_to_board(reconstruction_layer_board[i].numpy())\n",
        "  b_DT = numpy_to_board(de_normalisation(DT)[i].numpy())\n",
        "  cas_a_invalide += int(b.status() != chess.STATUS_VALID)\n",
        "  if b_DT.status() != chess.STATUS_VALID:\n",
        "    cas_a_invalide_DT += 1\n",
        "    invalide_board_DT.append(b_DT)\n",
        "  cas_a_different += int(b.board_fen() != dataset_config.memory_Boards[len(DA)+i].board_fen())"
      ],
      "metadata": {
        "id": "KBnPU5tu_UA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(invalide_board_DT[30].status())\n",
        "invalide_board_DT[30]"
      ],
      "metadata": {
        "id": "lg9Ysc40XTBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(de_normalisation(DT)[0][0])\n",
        "b_DT = numpy_to_board(de_normalisation(DT)[0].numpy())\n",
        "print(b_DT)\n",
        "int(dataset_config.memory_Boards[len(DA)+0].status() != chess.STATUS_VALID)\n",
        "b_DT"
      ],
      "metadata": {
        "id": "J5OgGk8tKt4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config.memory_Boards[len(DA)+0]"
      ],
      "metadata": {
        "id": "PBWXL6e_M9W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cas b : est-ce une configuration synthétique valide ?"
      ],
      "metadata": {
        "id": "ntdmI8K8-dbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Échantillonnage selon une loi normale\n",
        "    latent = torch.randn(Taille_DT, latent_dimension, device=device)\n",
        "\n",
        "    # Reconstruction\n",
        "    fake_config_board = de_normalisation(vae.decoder(latent).cpu())"
      ],
      "metadata": {
        "id": "DFM6fzKq9APj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_layer_board = predict_layer_board(fake_config_board, p=seuil_proba_prediction)\n",
        "#pred_layer_board[4]"
      ],
      "metadata": {
        "id": "ZIsuNWNXz7R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = numpy_to_board(pred_layer_board[5].numpy())\n",
        "print(b.status())\n",
        "b"
      ],
      "metadata": {
        "id": "TogeAhi7YO_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cas_b_invalide = 0\n",
        "for i in range(len(pred_layer_board)):\n",
        "  b = numpy_to_board(pred_layer_board[i].numpy())\n",
        "  cas_b_invalide += (b.status() != chess.STATUS_VALID)"
      ],
      "metadata": {
        "id": "s9UxQN-Yvfb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Résultats générations configurations"
      ],
      "metadata": {
        "id": "B9GtQpwou7Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"cas a : taux d'échec (validité) de {cas_a_invalide*100/len(reconstruction_layer_board)}%\")\n",
        "print(f\"cas a : taux de différence {cas_a_different*100/len(reconstruction_layer_board)}%\")\n",
        "print(f\"cas a : [DT]taux d'échec (validité) de {cas_a_invalide_DT*100/len(reconstruction_layer_board)}%\")"
      ],
      "metadata": {
        "id": "etOIgjAd_giy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"cas b : taux d'échec de {cas_b_invalide*100/len(pred_layer_board)}%\")"
      ],
      "metadata": {
        "id": "ZRYvSR7AyBoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tests"
      ],
      "metadata": {
        "id": "rhvFaOqubGxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(de_normalisation(normalisation(DT))[5][2])\n",
        "dataset_config.memory_Boards[10005]"
      ],
      "metadata": {
        "id": "C8aGbkEHwfD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_board = board_to_numpy(board)\n",
        "new_board = chess.Board(\"r2q1rk1/1p1nbppp/p1n1p3/2pp4/3P4/2N1P3/PPPBBPPP/1R1Q1RK1\")\n",
        "layer_new_board = board_to_numpy(new_board)\n",
        "\n",
        "t1 = torch.Tensor(layer_board)\n",
        "t2 = torch.Tensor(layer_new_board)\n",
        "memory_D = torch.Tensor(size=(1000,6,8,8))\n",
        "memory_D[0] = t1\n",
        "memory_D[1] = t2\n",
        "\n",
        "memory_Boards = []\n",
        "memory_Boards.append(board)\n",
        "memory_Boards.append(new_board)\n",
        "\n",
        "print(board.board_fen())\n",
        "print(board)\n",
        "print(layer_board[5])\n",
        "print(new_board)\n",
        "print(layer_new_board[0])\n",
        "print(new_board.status())\n",
        "memory_Boards[1]"
      ],
      "metadata": {
        "id": "VnYfi21m3Uwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tests RLC \"capture\""
      ],
      "metadata": {
        "id": "7Tn-Jn_G6LSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apprentissage Agents RL (Reinforce et Q-Learning)"
      ],
      "metadata": {
        "id": "KLFASa3jB5C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow._api.v2.compat.v1 as tf\n",
        "import time\n",
        "\n",
        "import chess.engine\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "board = Board()\n",
        "agent = Agent(network='conv_pg',lr=0.1)\n",
        "R = Reinforce(agent,board, opponent=engine)\n",
        "\n",
        "start = time.time()\n",
        "pgn = R.learn(iters=8000)\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "\n",
        "engine.quit()"
      ],
      "metadata": {
        "id": "omKcb0ej6Qqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow._api.v2.compat.v1 as tf\n",
        "import time\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "board = Board()\n",
        "agent_qlearning = Agent(network='conv',gamma=0.5,lr=0.08)\n",
        "Q = Q_learning(agent_qlearning,board)\n",
        "\n",
        "start = time.time()\n",
        "pgn = Q.learn(iters=750, windows_r_average=100)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "tAOrqCdNthTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parties agent RLC (Reinforce) contre lui même"
      ],
      "metadata": {
        "id": "TlrRMs5t02Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_games_2(rounds=1000):\n",
        "  round_success_w = 0\n",
        "  round_success_b = 0\n",
        "  for i in range(rounds):\n",
        "      max_iter = 1000\n",
        "      k = 0\n",
        "      board = chess.Board()\n",
        "      while not board.is_game_over() and k < max_iter:\n",
        "          k += 1\n",
        "          if(board.turn): # blanc, notre agent joue\n",
        "            white_move = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(white_move)\n",
        "          else: # black, c'est encore le même agent qui joue\n",
        "            result = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(result)\n",
        "      success_w = 1 if board.result()==\"1-0\" else 0\n",
        "      success_b = 1 if board.result()==\"0-1\" else 0\n",
        "      round_success_w += success_w\n",
        "      round_success_b += success_b\n",
        "  return round_success_w/rounds, round_success_b/rounds\n",
        "\n",
        "rounds = 1000\n",
        "\n",
        "round_success_w, round_success_b = run_games_2(rounds=rounds)"
      ],
      "metadata": {
        "id": "-ZrKeCCD1IDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "style.use('seaborn-pastel')\n",
        "\n",
        "Class = [\"1\"]\n",
        "w = 0.6\n",
        "round_success_nul = 1 - (round_success_w + round_success_b)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "color = 'steelblue'\n",
        "plt.bar(Class, round_success_w, w, label='blanc gagnant', color = color,alpha=0.3)\n",
        "plt.bar(Class, round_success_nul, w, bottom=round_success_w, label='match nul', color = color, alpha=0.7)\n",
        "plt.bar(Class, round_success_b, w, bottom=round_success_nul+round_success_w, label='noir gagnant', color = color)\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Répartition des victoires\")\n",
        "plt.title(\"Résultats RLC Reinforce vs RLC Reinforce - 1000 parties\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(round_success_w,round_success_nul, round_success_b)"
      ],
      "metadata": {
        "id": "6tGf9HlJ13Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parties agent RLC (Reinforce) contre agent aléatoire"
      ],
      "metadata": {
        "id": "mw749ho_-o83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_games_3(rounds=1000):\n",
        "  round_success_w = 0\n",
        "  round_success_b = 0\n",
        "  for i in range(rounds):\n",
        "      max_iter = 1000\n",
        "      k = 0\n",
        "      board = chess.Board()\n",
        "      while not board.is_game_over() and k < max_iter:\n",
        "          k += 1\n",
        "          if(board.turn): # blanc, un agent aléatoire pur\n",
        "            white_move = get_random_action(board)\n",
        "            board.push(white_move)\n",
        "          else: # black, c'est notre agent (un petit désavantage)\n",
        "            result = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(result)\n",
        "      success_w = 1 if board.result()==\"1-0\" else 0\n",
        "      success_b = 1 if board.result()==\"0-1\" else 0\n",
        "      round_success_w += success_w\n",
        "      round_success_b += success_b\n",
        "  return round_success_w/rounds, round_success_b/rounds\n",
        "\n",
        "rounds = 1000\n",
        "\n",
        "round_success_w, round_success_b = run_games_3(rounds=rounds)"
      ],
      "metadata": {
        "id": "gekx2y1l-qMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "style.use('seaborn-pastel')\n",
        "\n",
        "Class = [\"1\"]\n",
        "w = 0.6\n",
        "round_success_nul = 1 - (round_success_w + round_success_b)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "color = 'steelblue'\n",
        "plt.bar(Class, round_success_w, w, label='blanc gagnant', color = color,alpha=0.3)\n",
        "plt.bar(Class, round_success_nul, w, bottom=round_success_w, label='match nul', color = color, alpha=0.7)\n",
        "plt.bar(Class, round_success_b, w, bottom=round_success_nul+round_success_w, label='noir gagnant', color = color)\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Répartition des victoires\")\n",
        "plt.title(\"Résultats RLC Reinforce vs aléatoire - 1000 parties\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(round_success_w,round_success_nul, round_success_b)"
      ],
      "metadata": {
        "id": "oN30vjR1-3Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parties agent RLC (Reinforce) contre RLC (Q-learning)"
      ],
      "metadata": {
        "id": "vMWTXAIlPjta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_games_4(rounds=1000):\n",
        "  round_success_w = 0\n",
        "  round_success_b = 0\n",
        "  for i in range(rounds):\n",
        "      max_iter = 1000\n",
        "      k = 0\n",
        "      board = chess.Board()\n",
        "      while not board.is_game_over() and k < max_iter:\n",
        "          k += 1\n",
        "          if(board.turn): # blanc, notre agent RLC Reinforce\n",
        "            white_move = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(white_move)\n",
        "          else: # black, c'est notre agent RLC Q-learning\n",
        "            result = next_white_move(agent_qlearning, board, debug=False, best_probs=True)\n",
        "            board.push(result)\n",
        "      success_w = 1 if board.result()==\"1-0\" else 0\n",
        "      success_b = 1 if board.result()==\"0-1\" else 0\n",
        "      round_success_w += success_w\n",
        "      round_success_b += success_b\n",
        "  return round_success_w/rounds, round_success_b/rounds\n",
        "\n",
        "rounds = 1000\n",
        "\n",
        "round_success_w, round_success_b = run_games_4(rounds=rounds)"
      ],
      "metadata": {
        "id": "RdXtxR5-Pqay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "style.use('seaborn-pastel')\n",
        "\n",
        "Class = [\"1\"]\n",
        "w = 0.6\n",
        "round_success_nul = 1 - (round_success_w + round_success_b)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "color = 'steelblue'\n",
        "plt.bar(Class, round_success_w, w, label='blanc gagnant', color = color,alpha=0.3)\n",
        "plt.bar(Class, round_success_nul, w, bottom=round_success_w, label='match nul', color = color, alpha=0.7)\n",
        "plt.bar(Class, round_success_b, w, bottom=round_success_nul+round_success_w, label='noir gagnant', color = color)\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Répartition des victoires\")\n",
        "plt.title(\"Résultats RLC Reinforce vs RLC Q-Learning - 1000 parties\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(round_success_w,round_success_nul, round_success_b)"
      ],
      "metadata": {
        "id": "3_JcAlfxP-5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parties avec Agent RLC (Reinforce) en blanc et stockfish en noir"
      ],
      "metadata": {
        "id": "JxhkIV-NCHlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "rounds = 1000\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "def run_games(rounds=1000, time=0.1, depth=2, nodes=3):\n",
        "  round_success_w = 0\n",
        "  round_success_b = 0\n",
        "  for i in range(rounds):\n",
        "      max_iter = 1000\n",
        "      k = 0\n",
        "      board = chess.Board()\n",
        "      while not board.is_game_over() and k < max_iter:\n",
        "          k += 1\n",
        "          if(board.turn): # blanc, notre agent joue\n",
        "            white_move = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(white_move)\n",
        "          else: # black, c'est stockfish qui joue\n",
        "            result = engine.play(board, chess.engine.Limit(time=time, depth=depth, nodes=nodes))\n",
        "            board.push(result.move)\n",
        "      success_w = 1 if board.result()==\"1-0\" else 0\n",
        "      success_b = 1 if board.result()==\"0-1\" else 0\n",
        "      round_success_w += success_w\n",
        "      round_success_b += success_b\n",
        "  return round_success_w/rounds, round_success_b/rounds\n",
        "\n",
        "n = 4\n",
        "round_success_w = np.zeros(n)\n",
        "round_success_b = np.zeros(n)\n",
        "\n",
        "for i in range(n):\n",
        "  round_success_w[i], round_success_b[i] = run_games(rounds=rounds, time=0.1, depth=4, nodes=i+1)\n",
        "\n",
        "engine.quit()"
      ],
      "metadata": {
        "id": "-ambK5VL57cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "round_success_w_10_noeuds, round_success_b_10_noeuds = run_games(rounds=rounds, time=0.1, depth=4, nodes=10)\n",
        "engine.quit()"
      ],
      "metadata": {
        "id": "we1gljzoyRvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "style.use('seaborn-pastel')\n",
        "\n",
        "Class = [\"1 noeud\", \"2 noeuds\", \"3 noeuds\", \"4 noeuds\"]\n",
        "w = 0.6\n",
        "round_success_nul = 1 - (round_success_w + round_success_b)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "color = 'steelblue'\n",
        "plt.bar(Class, round_success_w, w, label='blanc gagnant', color = color,alpha=0.3)\n",
        "plt.bar(Class, round_success_nul, w, bottom=round_success_w, label='match nul', color = color, alpha=0.7)\n",
        "plt.bar(Class, round_success_b, w, bottom=round_success_nul+round_success_w, label='noir gagnant', color = color)\n",
        "plt.xlabel(\"Limites stockfish - noeuds (profondeur commune = 4)\")\n",
        "plt.ylabel(\"Répartition des victoires\")\n",
        "plt.title(\"Résultats RLC Reinforce vs Stockfish (limité) - 1000 parties\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(round_success_w,round_success_nul, round_success_b)"
      ],
      "metadata": {
        "id": "88L9cbo1h8_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apprentissage Agent A2C"
      ],
      "metadata": {
        "id": "MtDa29XlCSSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "from chess.pgn import Game\n",
        "\n",
        "board = Board()\n",
        "critic = Agent(network='conv',lr=0.1)\n",
        "critic.fix_model()\n",
        "actor = Agent(network='conv_pg',lr=0.3)\n",
        "R = ActorCritic(actor, critic,board)\n",
        "pgn = R.learn(iters=1000)"
      ],
      "metadata": {
        "id": "n4wPibNvwEA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tests RLC \"real\""
      ],
      "metadata": {
        "id": "E8oyWcPl2oA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RLC \"real\"\n",
        "[Reprise du code de la librairie RLC de arjangroen](https://github.com/arjangroen/RLC/tree/master/RLC/real_chess)"
      ],
      "metadata": {
        "id": "T6DV3sCmwcvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agent\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras.layers import Input, Dense, Flatten, Concatenate, Conv2D, Dropout\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.models import Model, clone_model, load_model\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RandomAgent(object):\n",
        "\n",
        "    def __init__(self, color=1):\n",
        "        self.color = color\n",
        "\n",
        "    def predict(self, board_layer):\n",
        "        return np.random.randint(-5, 5) / 5\n",
        "\n",
        "    def select_move(self, board):\n",
        "        moves = [x for x in board.generate_legal_moves()]\n",
        "        return np.random.choice(moves)\n",
        "\n",
        "\n",
        "class GreedyAgent(object):\n",
        "\n",
        "    def __init__(self, color=-1):\n",
        "        self.color = color\n",
        "\n",
        "    def predict(self, layer_board, noise=True):\n",
        "        layer_board1 = layer_board[0, :, :, :]\n",
        "        pawns = 1 * np.sum(layer_board1[0, :, :])\n",
        "        rooks = 5 * np.sum(layer_board1[1, :, :])\n",
        "        minor = 3 * np.sum(layer_board1[2:4, :, :])\n",
        "        queen = 9 * np.sum(layer_board1[4, :, :])\n",
        "\n",
        "        maxscore = 40\n",
        "        material = pawns + rooks + minor + queen\n",
        "        board_value = self.color * material / maxscore\n",
        "        if noise:\n",
        "            added_noise = np.random.randn() / 1e3\n",
        "        return board_value + added_noise\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, lr=0.003, network='big'):\n",
        "        self.optimizer = RMSprop(learning_rate=lr)\n",
        "        self.model = Model()\n",
        "        self.proportional_error = False\n",
        "        if network == 'simple':\n",
        "            self.init_simple_network()\n",
        "        elif network == 'super_simple':\n",
        "            self.init_super_simple_network()\n",
        "        elif network == 'alt':\n",
        "            self.init_altnet()\n",
        "        elif network == 'big':\n",
        "            self.init_bignet()\n",
        "        else:\n",
        "            self.init_network()\n",
        "\n",
        "    def fix_model(self):\n",
        "        \"\"\"\n",
        "        The fixed model is the model used for bootstrapping\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "\n",
        "        self.fixed_model = clone_model(self.model)\n",
        "        self.fixed_model.compile(optimizer=self.optimizer, loss='mse', metrics=['mae'])\n",
        "        self.fixed_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def init_network(self):\n",
        "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
        "\n",
        "        openfile = Conv2D(3, (8, 1), padding='valid', activation='relu', name='fileconv')(layer_state)  # 3,8,1\n",
        "        openrank = Conv2D(3, (1, 8), padding='valid', activation='relu', name='rankconv')(layer_state)  # 3,1,8\n",
        "        quarters = Conv2D(3, (4, 4), padding='valid', activation='relu', name='quarterconv', strides=(4, 4))(\n",
        "            layer_state)  # 3,2,2\n",
        "        large = Conv2D(8, (6, 6), padding='valid', activation='relu', name='largeconv')(layer_state)  # 8,2,2\n",
        "\n",
        "        board1 = Conv2D(16, (3, 3), padding='valid', activation='relu', name='board1')(layer_state)  # 16,6,6\n",
        "        board2 = Conv2D(20, (3, 3), padding='valid', activation='relu', name='board2')(board1)  # 20,4,4\n",
        "        board3 = Conv2D(24, (3, 3), padding='valid', activation='relu', name='board3')(board2)  # 24,2,2\n",
        "\n",
        "        flat_file = Flatten()(openfile)\n",
        "        flat_rank = Flatten()(openrank)\n",
        "        flat_quarters = Flatten()(quarters)\n",
        "        flat_large = Flatten()(large)\n",
        "\n",
        "        flat_board = Flatten()(board1)\n",
        "        flat_board3 = Flatten()(board3)\n",
        "\n",
        "        dense1 = Concatenate(name='dense_bass')(\n",
        "            [flat_file, flat_rank, flat_quarters, flat_large, flat_board, flat_board3])\n",
        "        dropout1 = Dropout(rate=0.1)(dense1)\n",
        "        dense2 = Dense(128, activation='sigmoid')(dropout1)\n",
        "        dense3 = Dense(64, activation='sigmoid')(dense2)\n",
        "        dropout3 = Dropout(rate=0.1)(dense3, training=True)\n",
        "        dense4 = Dense(32, activation='sigmoid')(dropout3)\n",
        "        dropout4 = Dropout(rate=0.1)(dense4, training=True)\n",
        "\n",
        "        value_head = Dense(1)(dropout4)\n",
        "        self.model = Model(inputs=layer_state,\n",
        "                           outputs=[value_head])\n",
        "        self.model.compile(optimizer=self.optimizer,\n",
        "                           loss=[mean_squared_error]\n",
        "                           )\n",
        "\n",
        "    def init_simple_network(self):\n",
        "\n",
        "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
        "        conv1 = Conv2D(8, (3, 3), activation='sigmoid')(layer_state)\n",
        "        conv2 = Conv2D(6, (3, 3), activation='sigmoid')(conv1)\n",
        "        conv3 = Conv2D(4, (3, 3), activation='sigmoid')(conv2)\n",
        "        flat4 = Flatten()(conv3)\n",
        "        dense5 = Dense(24, activation='sigmoid')(flat4)\n",
        "        dense6 = Dense(8, activation='sigmoid')(dense5)\n",
        "        value_head = Dense(1)(dense6)\n",
        "\n",
        "        self.model = Model(inputs=layer_state,\n",
        "                           outputs=value_head)\n",
        "        self.model.compile(optimizer=self.optimizer,\n",
        "                           loss=mean_squared_error\n",
        "                           )\n",
        "\n",
        "    def init_super_simple_network(self):\n",
        "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
        "        conv1 = Conv2D(8, (3, 3), activation='sigmoid')(layer_state)\n",
        "        flat4 = Flatten()(conv1)\n",
        "        dense5 = Dense(10, activation='sigmoid')(flat4)\n",
        "        value_head = Dense(1)(dense5)\n",
        "\n",
        "        self.model = Model(inputs=layer_state,\n",
        "                           outputs=value_head)\n",
        "        self.model.compile(optimizer=self.optimizer,\n",
        "                           loss=mean_squared_error\n",
        "                           )\n",
        "\n",
        "    def init_altnet(self):\n",
        "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
        "        conv1 = Conv2D(6, (1, 1), activation='sigmoid')(layer_state)\n",
        "        flat2 = Flatten()(conv1)\n",
        "        dense3 = Dense(128, activation='sigmoid')(flat2)\n",
        "\n",
        "        value_head = Dense(1)(dense3)\n",
        "\n",
        "        self.model = Model(inputs=layer_state,\n",
        "                           outputs=value_head)\n",
        "        self.model.compile(optimizer=self.optimizer,\n",
        "                           loss=mean_squared_error\n",
        "                           )\n",
        "\n",
        "    def init_bignet(self):\n",
        "        layer_state = Input(shape=(8, 8, 8), name='state')\n",
        "        conv_xs = Conv2D(4, (1, 1), activation='relu')(layer_state)\n",
        "        conv_s = Conv2D(8, (2, 2), strides=(1, 1), activation='relu')(layer_state)\n",
        "        conv_m = Conv2D(12, (3, 3), strides=(2, 2), activation='relu')(layer_state)\n",
        "        conv_l = Conv2D(16, (4, 4), strides=(2, 2), activation='relu')(layer_state)\n",
        "        conv_xl = Conv2D(20, (8, 8), activation='relu')(layer_state)\n",
        "        conv_rank = Conv2D(3, (1, 8), activation='relu')(layer_state)\n",
        "        conv_file = Conv2D(3, (8, 1), activation='relu')(layer_state)\n",
        "\n",
        "        f_xs = Flatten()(conv_xs)\n",
        "        f_s = Flatten()(conv_s)\n",
        "        f_m = Flatten()(conv_m)\n",
        "        f_l = Flatten()(conv_l)\n",
        "        f_xl = Flatten()(conv_xl)\n",
        "        f_r = Flatten()(conv_rank)\n",
        "        f_f = Flatten()(conv_file)\n",
        "\n",
        "        dense1 = Concatenate(name='dense_bass')([f_xs, f_s, f_m, f_l, f_xl, f_r, f_f])\n",
        "        dense2 = Dense(256, activation='sigmoid')(dense1)\n",
        "        dense3 = Dense(128, activation='sigmoid')(dense2)\n",
        "        dense4 = Dense(56, activation='sigmoid')(dense3)\n",
        "        dense5 = Dense(64, activation='sigmoid')(dense4)\n",
        "        dense6 = Dense(32, activation='sigmoid')(dense5)\n",
        "\n",
        "        value_head = Dense(1)(dense6)\n",
        "\n",
        "        self.model = Model(inputs=layer_state,\n",
        "                           outputs=value_head)\n",
        "        self.model.compile(optimizer=self.optimizer,\n",
        "                           loss=mean_squared_error\n",
        "                           )\n",
        "\n",
        "    def predict_distribution(self, states, batch_size=256):\n",
        "        \"\"\"\n",
        "        :param states: list of distinct states\n",
        "        :param n:  each state is predicted n times\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        predictions_per_state = int(batch_size / len(states))\n",
        "        state_batch = []\n",
        "        for state in states:\n",
        "            state_batch = state_batch + [state for x in range(predictions_per_state)]\n",
        "\n",
        "        state_batch = np.stack(state_batch, axis=0)\n",
        "        predictions = self.model.predict(state_batch)\n",
        "        predictions = predictions.reshape(len(states), predictions_per_state)\n",
        "        mean_pred = np.mean(predictions, axis=1)\n",
        "        std_pred = np.std(predictions, axis=1)\n",
        "        upper_bound = mean_pred + 2 * std_pred\n",
        "\n",
        "        return mean_pred, std_pred, upper_bound\n",
        "\n",
        "    def predict(self, board_layer):\n",
        "        return self.model.predict(board_layer)\n",
        "\n",
        "    def TD_update(self, states, rewards, sucstates, episode_active, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Update the SARSA-network using samples from the minibatch\n",
        "        Args:\n",
        "            minibatch: list\n",
        "                The minibatch contains the states, moves, rewards and new states.\n",
        "\n",
        "        Returns:\n",
        "            td_errors: np.array\n",
        "                array of temporal difference errors\n",
        "\n",
        "        \"\"\"\n",
        "        suc_state_values = self.fixed_model.predict(sucstates)\n",
        "        V_target = np.array(rewards) + np.array(episode_active) * gamma * np.squeeze(suc_state_values)\n",
        "        # Perform a step of minibatch Gradient Descent.\n",
        "        self.model.fit(x=states, y=V_target, epochs=1, verbose=0)\n",
        "\n",
        "        V_state = self.model.predict(states)  # the expected future returns\n",
        "        td_errors = V_target - np.squeeze(V_state)\n",
        "\n",
        "        return td_errors\n",
        "\n",
        "    def MC_update(self, states, returns):\n",
        "        \"\"\"\n",
        "        Update network using a monte carlo playout\n",
        "        Args:\n",
        "            states: starting states\n",
        "            returns: discounted future rewards\n",
        "\n",
        "        Returns:\n",
        "            td_errors: np.array\n",
        "                array of temporal difference errors\n",
        "        \"\"\"\n",
        "        self.model.fit(x=states, y=returns, epochs=0, verbose=0)\n",
        "        V_state = np.squeeze(self.model.predict(states))\n",
        "        td_errors = returns - V_state\n",
        "\n",
        "        return td_errors"
      ],
      "metadata": {
        "id": "XZ8m4N1pwnbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Environnement\n",
        "import chess\n",
        "import numpy as np\n",
        "\n",
        "mapper = {}\n",
        "mapper[\"p\"] = 0\n",
        "mapper[\"r\"] = 1\n",
        "mapper[\"n\"] = 2\n",
        "mapper[\"b\"] = 3\n",
        "mapper[\"q\"] = 4\n",
        "mapper[\"k\"] = 5\n",
        "mapper[\"P\"] = 0\n",
        "mapper[\"R\"] = 1\n",
        "mapper[\"N\"] = 2\n",
        "mapper[\"B\"] = 3\n",
        "mapper[\"Q\"] = 4\n",
        "mapper[\"K\"] = 5\n",
        "\n",
        "\n",
        "class Board(object):\n",
        "\n",
        "    def __init__(self, opposing_agent, FEN=None, capture_reward_factor=0.01):\n",
        "        \"\"\"\n",
        "        Chess Board Environment\n",
        "        Args:\n",
        "            FEN: str\n",
        "                Starting FEN notation, if None then start in the default chess position\n",
        "            capture_reward_factor: float [0,inf]\n",
        "                reward for capturing a piece. Multiply material gain by this number. 0 for normal chess.\n",
        "        \"\"\"\n",
        "        self.FEN = FEN\n",
        "        self.capture_reward_factor = capture_reward_factor\n",
        "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        self.init_layer_board()\n",
        "        self.opposing_agent = opposing_agent\n",
        "\n",
        "    def init_layer_board(self):\n",
        "        \"\"\"\n",
        "        Initalize the numerical representation of the environment\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        for i in range(64):\n",
        "            row = i // 8\n",
        "            col = i % 8\n",
        "            piece = self.board.piece_at(i)\n",
        "            if piece == None:\n",
        "                continue\n",
        "            elif piece.symbol().isupper():\n",
        "                sign = 1\n",
        "            else:\n",
        "                sign = -1\n",
        "            layer = mapper[piece.symbol()]\n",
        "            self.layer_board[layer, row, col] = sign\n",
        "            self.layer_board[6, :, :] = 1 / self.board.fullmove_number\n",
        "        if self.board.turn:\n",
        "            self.layer_board[6, 0, :] = 1\n",
        "        else:\n",
        "            self.layer_board[6, 0, :] = -1\n",
        "        self.layer_board[7, :, :] = 1\n",
        "\n",
        "    def update_layer_board(self, move=None):\n",
        "        self._prev_layer_board = self.layer_board.copy()\n",
        "        self.init_layer_board()\n",
        "\n",
        "    def pop_layer_board(self):\n",
        "        self.layer_board = self._prev_layer_board.copy()\n",
        "        self._prev_layer_board = None\n",
        "\n",
        "    def step(self, action, test=True):\n",
        "        \"\"\"\n",
        "        Run a step\n",
        "        Args:\n",
        "            action: python chess move\n",
        "        Returns:\n",
        "            epsiode end: Boolean\n",
        "                Whether the episode has ended\n",
        "            reward: float\n",
        "                Difference in material value after the move\n",
        "        \"\"\"\n",
        "        piece_balance_before = self.get_material_value()\n",
        "        self.board.push(action)\n",
        "        self.update_layer_board(action)\n",
        "        piece_balance_after = self.get_material_value()\n",
        "        auxiliary_reward = (piece_balance_after - piece_balance_before) * self.capture_reward_factor\n",
        "        result = self.board.result()\n",
        "        if result == \"*\":\n",
        "            reward = 0\n",
        "            episode_end = False\n",
        "        elif result == \"1-0\":\n",
        "            reward = 1\n",
        "            episode_end = True\n",
        "        elif result == \"0-1\":\n",
        "            reward = -1\n",
        "            episode_end = True\n",
        "        elif result == \"1/2-1/2\":\n",
        "            reward = 0\n",
        "            episode_end = True\n",
        "        reward += auxiliary_reward\n",
        "\n",
        "        return episode_end, reward\n",
        "\n",
        "    def get_random_action(self):\n",
        "        \"\"\"\n",
        "        Sample a random action\n",
        "        Returns: move\n",
        "            A legal python chess move.\n",
        "\n",
        "        \"\"\"\n",
        "        legal_moves = [x for x in self.board.generate_legal_moves()]\n",
        "        legal_moves = np.random.choice(legal_moves)\n",
        "        return legal_moves\n",
        "\n",
        "    def project_legal_moves(self):\n",
        "        \"\"\"\n",
        "        Create a mask of legal actions\n",
        "        Returns: np.ndarray with shape (64,64)\n",
        "        \"\"\"\n",
        "        self.action_space = np.zeros(shape=(64, 64))\n",
        "        moves = [[x.from_square, x.to_square] for x in self.board.generate_legal_moves()]\n",
        "        for move in moves:\n",
        "            self.action_space[move[0], move[1]] = 1\n",
        "        return self.action_space\n",
        "\n",
        "    def get_material_value(self):\n",
        "        \"\"\"\n",
        "        Sums up the material balance using Reinfield values\n",
        "        Returns: The material balance on the board\n",
        "        \"\"\"\n",
        "        pawns = 1 * np.sum(self.layer_board[0, :, :])\n",
        "        rooks = 5 * np.sum(self.layer_board[1, :, :])\n",
        "        minor = 3 * np.sum(self.layer_board[2:4, :, :])\n",
        "        queen = 9 * np.sum(self.layer_board[4, :, :])\n",
        "        return pawns + rooks + minor + queen\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "        self.init_layer_board()"
      ],
      "metadata": {
        "id": "5rqwoA-4xKs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tree\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(x, temperature=1):\n",
        "    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n",
        "\n",
        "\n",
        "class Node(object):\n",
        "\n",
        "    def __init__(self, board=None, parent=None, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Game Node for Monte Carlo Tree Search\n",
        "        Args:\n",
        "            board: the chess board\n",
        "            parent: the parent node\n",
        "            gamma: the discount factor\n",
        "        \"\"\"\n",
        "        self.children = {}  # Child nodes\n",
        "        self.board = board  # Chess board\n",
        "        self.parent = parent\n",
        "        self.values = []  # reward + Returns\n",
        "        self.gamma = gamma\n",
        "        self.starting_value = 0\n",
        "\n",
        "    def update_child(self, move, Returns):\n",
        "        \"\"\"\n",
        "        Update a child with a simulation result\n",
        "        Args:\n",
        "            move: The move that leads to the child\n",
        "            Returns: the reward of the move and subsequent returns\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        child = self.children[move]\n",
        "        child.values.append(Returns)\n",
        "\n",
        "    def update(self, Returns=None):\n",
        "        \"\"\"\n",
        "        Update a node with observed Returns\n",
        "        Args:\n",
        "            Returns: Future returns\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if Returns:\n",
        "            self.values.append(Returns)\n",
        "\n",
        "    def select(self, color=1):\n",
        "        \"\"\"\n",
        "        Use Thompson sampling to select the best child node\n",
        "        Args:\n",
        "            color: Whether to select for white or black\n",
        "\n",
        "        Returns:\n",
        "            (node, move)\n",
        "            node: the selected node\n",
        "            move: the selected move\n",
        "        \"\"\"\n",
        "        assert color == 1 or color == -1, \"color has to be white (1) or black (-1)\"\n",
        "        if self.children:\n",
        "            max_sample = np.random.choice(color * np.array(self.values))\n",
        "            max_move = None\n",
        "            for move, child in self.children.items():\n",
        "                child_sample = np.random.choice(color * np.array(child.values))\n",
        "                if child_sample > max_sample:\n",
        "                    max_sample = child_sample\n",
        "                    max_move = move\n",
        "            if max_move:\n",
        "                return self.children[max_move], max_move\n",
        "            else:\n",
        "                return self, None\n",
        "        else:\n",
        "            return self, None\n",
        "\n",
        "    def simulate(self, model, env, depth=0, max_depth=4, random=False, temperature=1):\n",
        "        \"\"\"\n",
        "        Recursive Monte Carlo Playout\n",
        "        Args:\n",
        "            model: The model used for bootstrap estimation\n",
        "            env: the chess environment\n",
        "            depth: The recursion depth\n",
        "            max_depth: How deep to search\n",
        "            temperature: softmax temperature\n",
        "\n",
        "        Returns:\n",
        "            Playout result.\n",
        "        \"\"\"\n",
        "        board_in = env.board.fen()\n",
        "        if env.board.turn and random:\n",
        "            move = np.random.choice([x for x in env.board.generate_legal_moves()])\n",
        "        else:\n",
        "            successor_values = []\n",
        "            for move in env.board.generate_legal_moves():\n",
        "                episode_end, reward = env.step(move)\n",
        "                result = env.board.result()\n",
        "\n",
        "                if (result == \"1-0\" and env.board.turn) or (\n",
        "                        result == \"0-1\" and not env.board.turn):\n",
        "                    env.board.pop()\n",
        "                    env.init_layer_board()\n",
        "                    break\n",
        "                else:\n",
        "                    if env.board.turn:\n",
        "                        sucval = reward + self.gamma * np.squeeze(\n",
        "                            model.predict(np.expand_dims(env.layer_board, axis=0)))\n",
        "                    else:\n",
        "                        sucval = np.squeeze(env.opposing_agent.predict(np.expand_dims(env.layer_board, axis=0)))\n",
        "                    successor_values.append(sucval)\n",
        "                    env.board.pop()\n",
        "                    env.init_layer_board()\n",
        "\n",
        "            if not episode_end:\n",
        "                if env.board.turn:\n",
        "                    move_probas = softmax(np.array(successor_values), temperature=temperature)\n",
        "                    moves = [x for x in env.board.generate_legal_moves()]\n",
        "                else:\n",
        "                    move_probas = np.zeros(len(successor_values))\n",
        "                    move_probas[np.argmax(successor_values)] = 1\n",
        "                    moves = [x for x in env.board.generate_legal_moves()]\n",
        "                if len(moves) == 1:\n",
        "                    move = moves[0]\n",
        "                else:\n",
        "                    move = np.random.choice(moves, p=np.squeeze(move_probas))\n",
        "\n",
        "        episode_end, reward = env.step(move)\n",
        "\n",
        "        if episode_end:\n",
        "            Returns = reward\n",
        "        elif depth >= max_depth:  # Bootstrap the Monte Carlo Playout\n",
        "            Returns = reward + self.gamma * np.squeeze(model.predict(np.expand_dims(env.layer_board, axis=0)))\n",
        "        else:  # Recursively continue\n",
        "            Returns = reward + self.gamma * self.simulate(model, env, depth=depth + 1,temperature=temperature)\n",
        "\n",
        "        env.board.pop()\n",
        "        env.init_layer_board()\n",
        "\n",
        "        board_out = env.board.fen()\n",
        "        assert board_in == board_out\n",
        "\n",
        "        if depth == 0:\n",
        "            return Returns, move\n",
        "        else:\n",
        "            noise = np.random.randn() / 1e6\n",
        "            return Returns + noise"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MvGtDj8-xnDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Learn\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import gc\n",
        "\n",
        "\n",
        "def softmax(x, temperature=1):\n",
        "    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "class TD_search(object):\n",
        "\n",
        "    def __init__(self, env, agent, gamma=0.9, search_time=1, memsize=2000, batch_size=256, temperature=1):\n",
        "        \"\"\"\n",
        "        Chess algorithm that combines bootstrapped monte carlo tree search with Q Learning\n",
        "        Args:\n",
        "            env: RLC chess environment\n",
        "            agent: RLC chess agent\n",
        "            gamma: discount factor\n",
        "            search_time: maximum time spent doing tree search\n",
        "            memsize: Amount of training samples to keep in-memory\n",
        "            batch_size: Size of the training batches\n",
        "            temperature: softmax temperature for mcts\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.tree = Node(self.env)\n",
        "        self.gamma = gamma\n",
        "        self.memsize = memsize\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.reward_trace = []  # Keeps track of the rewards\n",
        "        self.piece_balance_trace = []  # Keep track of the material value on the board\n",
        "        self.ready = False  # Whether to start training\n",
        "        self.search_time = search_time\n",
        "        self.min_sim_count = 10\n",
        "\n",
        "        self.mem_state = np.zeros(shape=(1, 8, 8, 8))\n",
        "        self.mem_sucstate = np.zeros(shape=(1, 8, 8, 8))\n",
        "        self.mem_reward = np.zeros(shape=(1))\n",
        "        self.mem_error = np.zeros(shape=(1))\n",
        "        self.mem_episode_active = np.ones(shape=(1))\n",
        "\n",
        "    def learn(self, iters=40, c=5, timelimit_seconds=3600, maxiter=80):\n",
        "        \"\"\"\n",
        "        Start Reinforcement Learning Algorithm\n",
        "        Args:\n",
        "            iters: maximum amount of iterations to train\n",
        "            c: model update rate (once every C games)\n",
        "            timelimit_seconds: maximum training time\n",
        "            maxiter: Maximum duration of a game, in halfmoves\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        starttime = time.time()\n",
        "        for k in range(iters):\n",
        "            self.env.reset()\n",
        "            if k % c == 0:\n",
        "                self.agent.fix_model()\n",
        "                print(\"iter\", k)\n",
        "            if k > c:\n",
        "                self.ready = True\n",
        "            self.play_game(k, maxiter=maxiter)\n",
        "            if starttime + timelimit_seconds < time.time():\n",
        "                break\n",
        "        return self.env.board\n",
        "\n",
        "    def play_game(self, k, maxiter=80):\n",
        "        \"\"\"\n",
        "        Play a chess game and learn from it\n",
        "        Args:\n",
        "            k: the play iteration number\n",
        "            maxiter: maximum duration of the game (halfmoves)\n",
        "\n",
        "        Returns:\n",
        "            board: Chess environment on terminal state\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "        tree = Node(self.env.board, gamma=self.gamma)  # Initialize the game tree\n",
        "\n",
        "        # Play a game of chess\n",
        "        while not episode_end:\n",
        "            state = np.expand_dims(self.env.layer_board.copy(), axis=0)\n",
        "            state_value = self.agent.predict(state)\n",
        "\n",
        "            # White's turn involves tree-search\n",
        "            if self.env.board.turn:\n",
        "\n",
        "                # Do a Monte Carlo Tree Search after game iteration k\n",
        "                start_mcts_after = -1\n",
        "                if k > start_mcts_after:\n",
        "                    tree = self.mcts(tree)\n",
        "                    # Step the best move\n",
        "                    max_move = None\n",
        "                    max_value = np.NINF\n",
        "                    for move, child in tree.children.items():\n",
        "                        sampled_value = np.mean(child.values)\n",
        "                        if sampled_value > max_value:\n",
        "                            max_value = sampled_value\n",
        "                            max_move = move\n",
        "                else:\n",
        "                    max_move = np.random.choice([move for move in self.env.board.generate_legal_moves()])\n",
        "\n",
        "            # Black's turn is myopic\n",
        "            else:\n",
        "                max_move = None\n",
        "                max_value = np.NINF\n",
        "                for move in self.env.board.generate_legal_moves():\n",
        "                    self.env.step(move)\n",
        "                    if self.env.board.result() == \"0-1\":\n",
        "                        max_move = move\n",
        "                        self.env.board.pop()\n",
        "                        self.env.init_layer_board()\n",
        "                        break\n",
        "                    successor_state_value_opponent = self.env.opposing_agent.predict(\n",
        "                        np.expand_dims(self.env.layer_board, axis=0))\n",
        "                    if successor_state_value_opponent > max_value:\n",
        "                        max_move = move\n",
        "                        max_value = successor_state_value_opponent\n",
        "\n",
        "                    self.env.board.pop()\n",
        "                    self.env.init_layer_board()\n",
        "\n",
        "            if not (self.env.board.turn and max_move not in tree.children.keys()) or not k > start_mcts_after:\n",
        "                tree.children[max_move] = Node(gamma=0.9, parent=tree)\n",
        "\n",
        "            episode_end, reward = self.env.step(max_move)\n",
        "\n",
        "            tree = tree.children[max_move]\n",
        "            tree.parent = None\n",
        "            #gc.collect()\n",
        "\n",
        "            sucstate = np.expand_dims(self.env.layer_board, axis=0)\n",
        "            new_state_value = self.agent.predict(sucstate)\n",
        "\n",
        "            error = reward + self.gamma * new_state_value - state_value\n",
        "            error = float(np.squeeze(error))\n",
        "\n",
        "            turncount += 1\n",
        "            if turncount > maxiter and not episode_end:\n",
        "                episode_end = True\n",
        "\n",
        "            episode_active = 0 if episode_end else 1\n",
        "\n",
        "            # construct training sample state, prediction, error\n",
        "            self.mem_state = np.append(self.mem_state, state, axis=0)\n",
        "            self.mem_reward = np.append(self.mem_reward, reward)\n",
        "            self.mem_sucstate = np.append(self.mem_sucstate, sucstate, axis=0)\n",
        "            self.mem_error = np.append(self.mem_error, error)\n",
        "            self.reward_trace = np.append(self.reward_trace, reward)\n",
        "            self.mem_episode_active = np.append(self.mem_episode_active, episode_active)\n",
        "\n",
        "            if self.mem_state.shape[0] > self.memsize:\n",
        "                self.mem_state = self.mem_state[1:]\n",
        "                self.mem_reward = self.mem_reward[1:]\n",
        "                self.mem_sucstate = self.mem_sucstate[1:]\n",
        "                self.mem_error = self.mem_error[1:]\n",
        "                self.mem_episode_active = self.mem_episode_active[1:]\n",
        "                #gc.collect()\n",
        "\n",
        "            if turncount % 10 == 0:\n",
        "                self.update_agent()\n",
        "\n",
        "        piece_balance = self.env.get_material_value()\n",
        "        self.piece_balance_trace.append(piece_balance)\n",
        "        print(\"game ended with result\", reward, \"and material balance\", piece_balance, \"in\", turncount, \"halfmoves\")\n",
        "\n",
        "        return self.env.board\n",
        "\n",
        "    def update_agent(self):\n",
        "        \"\"\"\n",
        "        Update the Agent with TD learning\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if self.ready:\n",
        "            choice_indices, states, rewards, sucstates, episode_active = self.get_minibatch()\n",
        "            td_errors = self.agent.TD_update(states, rewards, sucstates, episode_active, gamma=self.gamma)\n",
        "            self.mem_error[choice_indices.tolist()] = td_errors\n",
        "\n",
        "    def get_minibatch(self, prioritized=True):\n",
        "        \"\"\"\n",
        "        Get a mini batch of experience\n",
        "        Args:\n",
        "            prioritized:\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if prioritized:\n",
        "            sampling_priorities = np.abs(self.mem_error) + 1e-9\n",
        "        else:\n",
        "            sampling_priorities = np.ones(shape=self.mem_error.shape)\n",
        "        sampling_probs = sampling_priorities / np.sum(sampling_priorities)\n",
        "        sample_indices = [x for x in range(self.mem_state.shape[0])]\n",
        "        choice_indices = np.random.choice(sample_indices,\n",
        "                                          min(self.mem_state.shape[0],\n",
        "                                              self.batch_size),\n",
        "                                          p=np.squeeze(sampling_probs),\n",
        "                                          replace=False\n",
        "                                          )\n",
        "        states = self.mem_state[choice_indices]\n",
        "        rewards = self.mem_reward[choice_indices]\n",
        "        sucstates = self.mem_sucstate[choice_indices]\n",
        "        episode_active = self.mem_episode_active[choice_indices]\n",
        "\n",
        "        return choice_indices, states, rewards, sucstates, episode_active\n",
        "\n",
        "    def mcts(self, node):\n",
        "        \"\"\"\n",
        "        Run Monte Carlo Tree Search\n",
        "        Args:\n",
        "            node: A game state node object\n",
        "\n",
        "        Returns:\n",
        "            the node with playout sims\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        starttime = time.time()\n",
        "        sim_count = 0\n",
        "        board_in = self.env.board.fen()\n",
        "\n",
        "        # First make a prediction for each child state\n",
        "        for move in self.env.board.generate_legal_moves():\n",
        "            if move not in node.children.keys():\n",
        "                node.children[move] = Node(self.env.board, parent=node)\n",
        "\n",
        "            episode_end, reward = self.env.step(move)\n",
        "\n",
        "            if episode_end:\n",
        "                successor_state_value = 0\n",
        "            else:\n",
        "                successor_state_value = np.squeeze(\n",
        "                    self.agent.model.predict(np.expand_dims(self.env.layer_board, axis=0))\n",
        "                )\n",
        "\n",
        "            child_value = reward + self.gamma * successor_state_value\n",
        "\n",
        "            node.update_child(move, child_value)\n",
        "            self.env.board.pop()\n",
        "            self.env.init_layer_board()\n",
        "        if not node.values:\n",
        "            node.values = [0]\n",
        "\n",
        "        while starttime + self.search_time > time.time() or sim_count < self.min_sim_count:\n",
        "            depth = 0\n",
        "            color = 1\n",
        "            node_rewards = []\n",
        "\n",
        "            # Select the best node from where to start MCTS\n",
        "            while node.children:\n",
        "                node, move = node.select(color=color)\n",
        "                if not move:\n",
        "                    # No move means that the node selects itself, not a child node.\n",
        "                    break\n",
        "                else:\n",
        "                    depth += 1\n",
        "                    color = color * -1  # switch color\n",
        "                    episode_end, reward = self.env.step(move)  # Update the environment to reflect the node\n",
        "                    node_rewards.append(reward)\n",
        "                    # Check best node is terminal\n",
        "\n",
        "                    if self.env.board.result() == \"1-0\" and depth == 1:  # -> Direct win for white, no need for mcts.\n",
        "                        self.env.board.pop()\n",
        "                        self.env.init_layer_board()\n",
        "                        node.update(1)\n",
        "                        node = node.parent\n",
        "                        return node\n",
        "                    elif episode_end:  # -> if the explored tree leads to a terminal state, simulate from root.\n",
        "                        while node.parent:\n",
        "                            self.env.board.pop()\n",
        "                            self.env.init_layer_board()\n",
        "                            node = node.parent\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "            # Expand the game tree with a simulation\n",
        "            Returns, move = node.simulate(self.agent.fixed_model,\n",
        "                                          self.env,\n",
        "                                          temperature=self.temperature,\n",
        "                                          depth=0)\n",
        "            self.env.init_layer_board()\n",
        "\n",
        "            if move not in node.children.keys():\n",
        "                node.children[move] = Node(self.env.board, parent=node)\n",
        "\n",
        "            node.update_child(move, Returns)\n",
        "\n",
        "            # Return to root node and backpropagate Returns\n",
        "            while node.parent:\n",
        "                latest_reward = node_rewards.pop(-1)\n",
        "                Returns = latest_reward + self.gamma * Returns\n",
        "                node.update(Returns)\n",
        "                node = node.parent\n",
        "\n",
        "                self.env.board.pop()\n",
        "                self.env.init_layer_board()\n",
        "            sim_count += 1\n",
        "\n",
        "        board_out = self.env.board.fen()\n",
        "        assert board_in == board_out\n",
        "\n",
        "        return node"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p0fsFw_4xUgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apprentissage Agent TD_search"
      ],
      "metadata": {
        "id": "RXDgJx0U209Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "#from RLC.real_chess import agent, environment, learn, tree\n",
        "import chess\n",
        "from chess.pgn import Game\n",
        "\n",
        "opponent = GreedyAgent()\n",
        "env = Board(opponent, FEN=None)\n",
        "player = Agent(lr=0.01, network='alt')\n",
        "\n",
        "player.model.load_weights(\"./chekpoint\")\n",
        "\n",
        "learner = TD_search(env, player, gamma=0.9, search_time=1, memsize=128, batch_size=64, temperature=1)\n",
        "node = Node(learner.env.board, gamma=learner.gamma)\n",
        "#player.model.summary()\n",
        "\n",
        "learner.learn(iters=1000, timelimit_seconds=14400)\n",
        "\n",
        "reward_smooth = pd.DataFrame(learner.reward_trace)\n",
        "reward_smooth.rolling(window=500, min_periods=0).mean().plot(figsize=(8, 6),\n",
        "                                                             title='average performance over the last 3 episodes')\n",
        "plt.show()\n",
        "\n",
        "reward_smooth = pd.DataFrame(learner.piece_balance_trace)\n",
        "reward_smooth.rolling(window=100, min_periods=0).mean().plot(figsize=(8, 6),\n",
        "                                                             title='average piece balance over the last 3 episodes')\n",
        "plt.show()\n",
        "\n",
        "#pgn = Game.from_board(learner.env.board)\n",
        "#with open(\"rlc_pgn\", \"w\") as log:\n",
        "#    log.write(str(pgn))\n",
        "\n",
        "player.model.save_weights(\"./chekpoint\")"
      ],
      "metadata": {
        "id": "HoueeOI5259B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Partie avec Agent TD_search en blanc et stockfish en noir"
      ],
      "metadata": {
        "id": "VcKFfR_y4n-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "max_iter = 100\n",
        "k = 0\n",
        "\n",
        "board = chess.Board()\n",
        "\n",
        "env = Board(opponent)\n",
        "env.board = board\n",
        "\n",
        "while not board.is_game_over() and k < max_iter:\n",
        "    k += 1\n",
        "    if(board.turn): # blanc, notre agent joue\n",
        "      returns, white_move = node.simulate(player.model,\n",
        "                                          env,\n",
        "                                          temperature=learner.temperature,\n",
        "                                          depth=0, max_depth=6)\n",
        "      #print(white_move)\n",
        "      board.push(white_move)\n",
        "    else: # black, c'est stockfish qui joue\n",
        "      result = engine.play(board, chess.engine.Limit(time=0.1, depth=1, nodes=1))\n",
        "      board.push(result.move)\n",
        "\n",
        "engine.quit()\n",
        "\n",
        "print(board.status, k, board.result())\n",
        "board"
      ],
      "metadata": {
        "id": "C5xWGmos4zo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}