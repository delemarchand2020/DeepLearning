{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delemarchand2020/DeepLearning/blob/main/Projet_G%C3%A9n%C3%A9ration_de_sc%C3%A9narios_et_renforcement_Denis_Lemarchand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tppUeEAifNa"
      },
      "source": [
        "#Installation des librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YLg5zBhh9U1"
      },
      "outputs": [],
      "source": [
        "!pip install chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTZQo2ZTCNFb"
      },
      "outputs": [],
      "source": [
        "!pip install stockfish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzjINu9FFT-9"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/sh/75gzfgu7qo94pvh/AACk_w5M94GTwwhSItCqsemoa/Stockfish%205/stockfish-5-linux.zip\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1gnoo7zvmhn35gUy093Eltw6a-PWOIpwz' -O stockfish-5-linux.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vASiby5QhA5I"
      },
      "outputs": [],
      "source": [
        "!unzip -o stockfish-5-linux.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzmK4lPdF6vU"
      },
      "outputs": [],
      "source": [
        "!chmod +x stockfish-5-linux/Linux/stockfish_14053109_x64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaMdxwakGiHF"
      },
      "outputs": [],
      "source": [
        "!ls -l stockfish-5-linux/Linux/stockfish_14053109_x64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaLeav4sAC32"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOkzLx8ViyO_"
      },
      "source": [
        "##Tests python-chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITbZFvDXGbhh"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "max_iter = 1000\n",
        "k = 0\n",
        "\n",
        "board = chess.Board()\n",
        "while not board.is_game_over() and k < max_iter:\n",
        "    if(board.turn): #blanc\n",
        "      white_player = engine.play(board, chess.engine.Limit(time=0.01))#, depth=2, nodes=5))\n",
        "      board.push(white_player.move)\n",
        "    else:\n",
        "      black_player = engine.play(board, chess.engine.Limit(time=0.001))\n",
        "      board.push(black_player.move)\n",
        "    k += 1\n",
        "\n",
        "engine.quit()\n",
        "\n",
        "print(board.status, k, board.result())\n",
        "board_gagnant = board\n",
        "board_gagnant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HxGQlpx3-sn"
      },
      "source": [
        "##Fonctions utilitaires (issues de RLC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BROUmwy3BSfH"
      },
      "outputs": [],
      "source": [
        "def get_project_legal_moves(board):\n",
        "        \"\"\"\n",
        "        Create a mask of legal actions\n",
        "        Returns: np.ndarray with shape (64,64)\n",
        "        \"\"\"\n",
        "        action_space = np.zeros(shape=(64, 64))\n",
        "        moves = [[x.from_square, x.to_square] for x in board.generate_legal_moves()]\n",
        "        for move in moves:\n",
        "            action_space[move[0], move[1]] = 1\n",
        "        return action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqGIOzYMAvNK"
      },
      "outputs": [],
      "source": [
        "def get_layer_board(board):\n",
        "        \"\"\"\n",
        "        Initalize the numerical representation of the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        for i in range(64):\n",
        "            row = i // 8\n",
        "            col = i % 8\n",
        "            piece = board.piece_at(i)\n",
        "            if piece == None:\n",
        "                continue\n",
        "            elif piece.symbol().isupper():\n",
        "                sign = 1\n",
        "            else:\n",
        "                sign = -1\n",
        "            layer = mapper[piece.symbol()]\n",
        "            layer_board[layer, row, col] = sign\n",
        "        if board.turn:\n",
        "            layer_board[6, :, :] = 1 / board.fullmove_number\n",
        "        if board.can_claim_draw():\n",
        "            layer_board[7, :, :] = 1\n",
        "        return layer_board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjWPk8Ps-zrh"
      },
      "outputs": [],
      "source": [
        "def next_white_move(agent, board, debug=False, best_probs=False):        \n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  action_spaces = []\n",
        "\n",
        "  state = get_layer_board(board)\n",
        "  action_space = get_project_legal_moves(board) # The environment determines which moves are legal\n",
        "  action_probs = agent.model.predict([np.expand_dims(state, axis=0),\n",
        "                                       np.zeros((1, 1)),\n",
        "                                       action_space.reshape(1, 4096)])\n",
        "  action_probs = action_probs / action_probs.sum()\n",
        "  move = np.random.choice(range(4096), p=np.squeeze(action_probs))\n",
        "  if(debug):\n",
        "    print(move, np.argmax(action_probs, axis=1)[0])\n",
        "  if(best_probs):\n",
        "    move = np.argmax(action_probs, axis=1)[0]\n",
        "  move_from = move // 64\n",
        "  move_to = move % 64\n",
        "  moves = [x for x in board.generate_legal_moves() if \\\n",
        "           x.from_square == move_from and x.to_square == move_to]\n",
        "  assert len(moves) > 0  # should not be possible\n",
        "  if len(moves) > 1:\n",
        "    move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "  elif len(moves) == 1:\n",
        "    move = moves[0]\n",
        "  return move      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNPwBtHL_fHB"
      },
      "outputs": [],
      "source": [
        "def get_random_action(board):\n",
        "  legal_moves = [x for x in board.generate_legal_moves()]\n",
        "  legal_moves = np.random.choice(legal_moves)\n",
        "  return legal_moves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz1xU73NF1c7"
      },
      "source": [
        "#Génération des configurations de l’échiquier avec un $β$-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AmDigYcuZBQ"
      },
      "source": [
        "##Librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-7hvpJT7N2I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "use_gpu = False\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Exécution sur {device}\")\n",
        "\n",
        "# Imports des bibliothèques utiles\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNOEUNOavTxy"
      },
      "source": [
        "##Hyperparamètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES7qL0oq2OtW"
      },
      "source": [
        "Wout = (Win - K + 2P)/S + 1\n",
        "\n",
        "Hout = (Hin - K + 2P)/S + 1\n",
        "\n",
        "K:Kernel\n",
        "\n",
        "P:Padding\n",
        "\n",
        "S:Stride"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnLXeJmW3-Qz"
      },
      "outputs": [],
      "source": [
        "Kernel_size = 3\n",
        "Stride_size = 1\n",
        "Padding_size = 1\n",
        "N_channels_1 = 11\n",
        "N_channels_2 = 11\n",
        "\n",
        "seuil_proba_prediction = 0.2\n",
        "\n",
        "W_in = 8\n",
        "H_in = 8\n",
        "\n",
        "W_out = (W_in - Kernel_size + 2*Padding_size)/Stride_size + 1\n",
        "H_out = (H_in - Kernel_size + 2*Padding_size)/Stride_size + 1\n",
        "\n",
        "print(W_out,H_out)\n",
        "\n",
        "W_out = int(W_out)\n",
        "H_out = int(H_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meOZ6RgC2234"
      },
      "outputs": [],
      "source": [
        "beta = 2\n",
        "latent_dimension = 100\n",
        "batch_size = 128\n",
        "\n",
        "N_epochs = 20\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAhS_BVowFTS"
      },
      "outputs": [],
      "source": [
        "Taille_DA = 20000\n",
        "Taille_DT = 4000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNgGDN1vueVU"
      },
      "source": [
        "##Conversion échiquier <-> numpy board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfKlO_TMNilN"
      },
      "outputs": [],
      "source": [
        "def board_to_numpy(board):\n",
        "        \"\"\"\n",
        "        Initalize the numerical representation of the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        layer_board = np.zeros(shape=(6, 8, 8))\n",
        "        for i in range(64):\n",
        "            row = i // 8\n",
        "            col = i % 8\n",
        "            piece = board.piece_at(i)\n",
        "            if piece == None:\n",
        "                continue\n",
        "            elif piece.symbol().isupper():\n",
        "                sign = 1\n",
        "            else:\n",
        "                sign = -1\n",
        "            layer = mapper[piece.symbol()]\n",
        "            layer_board[layer, row, col] = sign\n",
        "        return layer_board\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7qyS8gLQrtO"
      },
      "outputs": [],
      "source": [
        "mapper = {}\n",
        "#pieces noires\n",
        "mapper[\"p\"] = 0 #pion \n",
        "mapper[\"r\"] = 1 #tour\n",
        "mapper[\"n\"] = 2 #cavalier\n",
        "mapper[\"b\"] = 3 #fou\n",
        "mapper[\"q\"] = 4 #reine\n",
        "mapper[\"k\"] = 5 #roi\n",
        "#pieces blanches\n",
        "mapper[\"P\"] = 0\n",
        "mapper[\"R\"] = 1\n",
        "mapper[\"N\"] = 2\n",
        "mapper[\"B\"] = 3\n",
        "mapper[\"Q\"] = 4\n",
        "mapper[\"K\"] = 5\n",
        "\n",
        "layer = {}\n",
        "layer[0] = chess.PAWN\n",
        "layer[1] = chess.ROOK\n",
        "layer[2] = chess.KNIGHT\n",
        "layer[3] = chess.BISHOP\n",
        "layer[4] = chess.QUEEN\n",
        "layer[5] = chess.KING\n",
        "\n",
        "def numpy_to_board(layer_board):\n",
        "    board = chess.Board.empty()\n",
        "    for l in range(6):\n",
        "       for row in range(8):\n",
        "          for col in range(8):\n",
        "             piece = layer_board[l][row][col]\n",
        "             if piece == 1: #blanche\n",
        "               c = chess.WHITE\n",
        "             elif piece == -1: #noire\n",
        "               c = chess.BLACK\n",
        "             else:\n",
        "               continue\n",
        "             p = chess.Piece(layer[l],c)\n",
        "             board.set_piece_at(row*8+col, piece=p)\n",
        "    return board      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpmEJ2rRulJW"
      },
      "source": [
        "##Préparation du dataset des configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YW4NaZDGMOL"
      },
      "outputs": [],
      "source": [
        "# Configurer une instance de stockfish, une zone mémoire D pour stocker les configurations réelles d’échiquiers.\n",
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "class DatasetConfigBoard(object):\n",
        "\n",
        "    def __init__(self, N=1000, N_test=200, alea=0):\n",
        "        self.N = N\n",
        "        self.memory_DA = torch.Tensor(size=(N,6,8,8))\n",
        "        self.memory_DT = torch.Tensor(size=(N_test,6,8,8))\n",
        "        self.memory_Boards = [] #pour tests\n",
        "        self.N_test = N_test\n",
        "        self.alea = alea\n",
        "\n",
        "        self.stat_nb_parties = 0\n",
        "        self.stat_nb_moyen_coups_par_parties = 0\n",
        "        self.nb_victoires_blancs = 0\n",
        "        self.nb_victoires_noirs = 0\n",
        "        self.nb_fois_atteinte_max_iter = 0\n",
        "\n",
        "        #self.nb_moyen_captures_blancs_par_parties = 0\n",
        "        #self.nb_moyen_captures_noirs_par_parties = 0\n",
        "\n",
        "    def generate(self, N, memory_D, memory_Boards, calculs_stats = False):\n",
        "        engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "        i = 0\n",
        "        while i < N:\n",
        "          max_iter = 1000\n",
        "          k = 0\n",
        "          board = chess.Board()\n",
        "          while not board.is_game_over() and k < max_iter and i < N:\n",
        "              if(board.turn): #blanc\n",
        "                white_player = engine.play(board, chess.engine.Limit(time=0.01))\n",
        "                board.push(white_player.move)\n",
        "              else:\n",
        "                black_player = engine.play(board, chess.engine.Limit(time=0.015))\n",
        "                board.push(black_player.move)\n",
        "              k += 1\n",
        "\n",
        "              if np.random.uniform() > self.alea or k < 20:\n",
        "                memory_D[i] = torch.Tensor(board_to_numpy(board))\n",
        "                memory_Boards.append(board.copy())\n",
        "                i += 1\n",
        "          \n",
        "          if calculs_stats:\n",
        "              self.stat_nb_parties += 1\n",
        "              self.stat_nb_moyen_coups_par_parties += k\n",
        "                        \n",
        "              if k == max_iter:\n",
        "                self.nb_fois_atteinte_max_iter += 1\n",
        "          \n",
        "              if board.result() == \"1-0\":\n",
        "                self.nb_victoires_blancs += 1\n",
        "\n",
        "              if board.result() == \"0-1\":\n",
        "                self.nb_victoires_noirs += 1\n",
        "\n",
        "        if calculs_stats:\n",
        "            self.stat_nb_moyen_coups_par_parties = self.stat_nb_moyen_coups_par_parties/self.stat_nb_parties\n",
        "        engine.quit()\n",
        "\n",
        "    def create_dataset(self):\n",
        "        self.generate(self.N, self.memory_DA, self.memory_Boards, calculs_stats = True)\n",
        "        self.generate(self.N_test, self.memory_DT, self.memory_Boards)\n",
        "        return self.memory_DA, self.memory_DT\n",
        "\n",
        "    def get_stats(self):\n",
        "        print(f\"nb_parties {self.stat_nb_parties}\")\n",
        "        print(f\"nb_moyen_coups_par_parties {self.stat_nb_moyen_coups_par_parties}\")\n",
        "        print(f\"nb_victoires_blancs {self.nb_victoires_blancs}\")\n",
        "        print(f\"nb_victoires_noirs {self.nb_victoires_noirs}\")\n",
        "        print(f\"nb_matchs_nuls {self.stat_nb_parties - self.nb_victoires_noirs - self.nb_victoires_blancs}\")\n",
        "        print(f\"nb_fois_atteinte_max_iter {self.nb_fois_atteinte_max_iter}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vc5YaFhubcd"
      },
      "outputs": [],
      "source": [
        "dataset_config = DatasetConfigBoard(N=Taille_DA, N_test=Taille_DT, alea=0.5)\n",
        "DA, DT = dataset_config.create_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auPkTU5Zjip2"
      },
      "outputs": [],
      "source": [
        "# normalisation : -1 -> 0, 0 -> 0.5, 1 -> 1\n",
        "# normalisation(x): (x + 1)/2\n",
        "def normalisation(x):\n",
        "  return (x+1)/2\n",
        "\n",
        "def de_normalisation(x):\n",
        "  return (2*x - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRjaH5MDkv4V"
      },
      "outputs": [],
      "source": [
        "DA_copy = DA\n",
        "DT_copy = DT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3VYbuOuk2on"
      },
      "outputs": [],
      "source": [
        "DA = normalisation(DA_copy)\n",
        "DT = normalisation(DT_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBRN8kxwvp1Q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(DA, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj6gCoiuyYZB"
      },
      "source": [
        "##Implémentation $\\beta$-VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne0AXC3QyeyZ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dimension):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = nn.Sequential(nn.Conv2d(6, N_channels_1, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Conv2d(N_channels_1, N_channels_2, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Flatten(),\n",
        "                                   )\n",
        "        self.linear1 = nn.Linear(in_features=(N_channels_2*W_out*H_out), out_features=latent_dimension)\n",
        "        self.linear2 = nn.Linear(in_features=(N_channels_2*W_out*H_out), out_features=latent_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x_mu = self.linear1(x)\n",
        "        x_logvar = self.linear2(x)\n",
        "        return x_mu, x_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dimension):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear = nn.Linear(in_features=latent_dimension, out_features=N_channels_2*W_out*H_out)\n",
        "        self.model = nn.Sequential(nn.ConvTranspose2d(N_channels_2, N_channels_1, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.ConvTranspose2d(N_channels_1, 6, kernel_size=Kernel_size, stride=Stride_size, padding=Padding_size),\n",
        "                                     nn.Sigmoid()\n",
        "                                    )\n",
        "\n",
        "    def forward(self, z):\n",
        "        hat_x = F.relu(self.linear(z))\n",
        "        hat_x = hat_x.view(-1, N_channels_2, W_out, H_out)\n",
        "        hat_x = self.model(hat_x)\n",
        "        return hat_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFzM_nDdysVN"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        z = self.latent_sample(latent_mu, latent_logvar)\n",
        "        hat_x = self.decoder(z)\n",
        "        return hat_x, latent_mu, latent_logvar\n",
        "\n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1X9V-KlzIf7"
      },
      "outputs": [],
      "source": [
        "def vae_loss(hat_x, x, mu, logvar):\n",
        "    reconstruction_loss = F.binary_cross_entropy(hat_x.view(-1, 6*8*8), x.view(-1, 6*8*8), reduction='sum')\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return reconstruction_loss + beta * kl_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTeHCTT7zKVF"
      },
      "source": [
        "##Apprentissage $\\beta$-VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx3voumzzPsr"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "def train_vae(net, train_dataset, epochs=10, learning_rate=1e-3, batch_size=128, device=device):\n",
        "    # Création du DataLoader pour charger les données\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    # Définition de l'algorithme d'optimisation (Adam, variante de la SGD)\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    # Choix de la fonction de coût\n",
        "    criterion = vae_loss\n",
        "    # Passe le modèle en mode \"apprentissage\"\n",
        "    net = net.to(device)\n",
        "    net = net.train()\n",
        "\n",
        "    t = trange(1, epochs + 1, desc=\"Entraînement du modèle\")\n",
        "    for epoch in t:\n",
        "        avg_loss = 0.\n",
        "        # Parcours du dataset pour une epoch\n",
        "        for images in tqdm(train_dataloader):\n",
        "            # les labels sont ignorés pour l'apprentissage de l'auto-encodeur\n",
        "\n",
        "            images = images.to(device)\n",
        "            # Calcul de la reconstruction\n",
        "            reconstructions, latent_mu, latent_logvar = net(images)\n",
        "            # Calcul de l'erreur\n",
        "            loss = criterion(reconstructions, images, latent_mu, latent_logvar)\n",
        "\n",
        "            # Rétropropagation du gradient\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Descente de gradient (une itération)\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "        avg_loss /= len(train_dataloader)\n",
        "        t.set_description(f\"Epoch {epoch}: loss = {avg_loss:.3f}\")\n",
        "    return net.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja-iuOntzYN6"
      },
      "outputs": [],
      "source": [
        "vae = VariationalAutoencoder(latent_dimension)\n",
        "train_vae(vae, DA, epochs=N_epochs, learning_rate=learning_rate, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhTm3l9Ku6oc"
      },
      "source": [
        "##Tests de génération de configurations synthétiques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBKupBu8-qbu"
      },
      "source": [
        "###Cas a : la reconstruction est-elle identique à l’entrée, est-elle valide ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeGg0gtV80Op"
      },
      "outputs": [],
      "source": [
        "def predict_layer_board(fake_config_board, p=0.5):\n",
        "  layer_board = torch.round(fake_config_board, decimals=2)\n",
        "  layer_board[layer_board>p] = 1\n",
        "  layer_board[layer_board<-p] = -1\n",
        "  layer_board[torch.abs(layer_board)!=1] = 0\n",
        "  return layer_board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTTwkd-F7kt5"
      },
      "outputs": [],
      "source": [
        "vae.eval()\n",
        "reconstruction,_,_ = vae(DT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWHerSLE8KjC"
      },
      "outputs": [],
      "source": [
        "reconstruction_config_board = de_normalisation(reconstruction.detach())\n",
        "reconstruction_layer_board = predict_layer_board(reconstruction_config_board, p=seuil_proba_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTX_MVx19U-x"
      },
      "outputs": [],
      "source": [
        "n = 354\n",
        "b = numpy_to_board(reconstruction_layer_board[n].numpy())\n",
        "print(b.status())\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDpmCV9x9uYl"
      },
      "outputs": [],
      "source": [
        "dataset_config.memory_Boards[len(DA)+n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvKFApLz__yX"
      },
      "outputs": [],
      "source": [
        "print(b.board_fen())\n",
        "print(dataset_config.memory_Boards[len(DA)+n].board_fen())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBnPU5tu_UA8"
      },
      "outputs": [],
      "source": [
        "cas_a_invalide = 0\n",
        "cas_a_invalide_DT = 0\n",
        "cas_a_different = 0\n",
        "\n",
        "invalide_board_DT = []\n",
        "\n",
        "for i in range(len(reconstruction_layer_board)):\n",
        "  b = numpy_to_board(reconstruction_layer_board[i].numpy())\n",
        "  b_DT = numpy_to_board(de_normalisation(DT)[i].numpy())\n",
        "  cas_a_invalide += int(b.status() != chess.STATUS_VALID)\n",
        "  if b_DT.status() != chess.STATUS_VALID:\n",
        "    cas_a_invalide_DT += 1\n",
        "    invalide_board_DT.append(b_DT)\n",
        "  cas_a_different += int(b.board_fen() != dataset_config.memory_Boards[len(DA)+i].board_fen())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg9Ysc40XTBY"
      },
      "outputs": [],
      "source": [
        "print(invalide_board_DT[30].status())\n",
        "invalide_board_DT[30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5OgGk8tKt4n"
      },
      "outputs": [],
      "source": [
        "print(de_normalisation(DT)[0][0])\n",
        "b_DT = numpy_to_board(de_normalisation(DT)[0].numpy())\n",
        "print(b_DT)\n",
        "int(dataset_config.memory_Boards[len(DA)+0].status() != chess.STATUS_VALID)\n",
        "b_DT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBWXL6e_M9W8"
      },
      "outputs": [],
      "source": [
        "dataset_config.memory_Boards[len(DA)+0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntdmI8K8-dbc"
      },
      "source": [
        "###Cas b : est-ce une configuration synthétique valide ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFM6fzKq9APj"
      },
      "outputs": [],
      "source": [
        "vae.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Échantillonnage selon une loi normale\n",
        "    latent = torch.randn(Taille_DT, latent_dimension, device=device)\n",
        "\n",
        "    # Reconstruction\n",
        "    fake_config_board = de_normalisation(vae.decoder(latent).cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIsuNWNXz7R9"
      },
      "outputs": [],
      "source": [
        "pred_layer_board = predict_layer_board(fake_config_board, p=0.21) #seuil_proba_prediction\n",
        "pred_layer_board[4][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TogeAhi7YO_o"
      },
      "outputs": [],
      "source": [
        "b = numpy_to_board(pred_layer_board[int(np.random.uniform(low=0,high=len(pred_layer_board)))].numpy())\n",
        "print(b.status())\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9UxQN-Yvfb5"
      },
      "outputs": [],
      "source": [
        "cas_b_invalide = 0\n",
        "valid_board_cas_b = []\n",
        "\n",
        "for i in range(len(pred_layer_board)):\n",
        "  b = numpy_to_board(pred_layer_board[i].numpy())\n",
        "  if b.status() != chess.STATUS_VALID:\n",
        "    cas_b_invalide += 1\n",
        "  else:\n",
        "    valid_board_cas_b.append(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPq8ozhPYC1J"
      },
      "outputs": [],
      "source": [
        "b = valid_board_cas_b[int(np.random.uniform(low=0,high=len(valid_board_cas_b)))]\n",
        "print(b.status())\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihin4PStPg3E"
      },
      "outputs": [],
      "source": [
        "cas_b_empty = 0\n",
        "last_i_not_empty = 0\n",
        "first_i_not_empty = 0\n",
        "for i in range(len(pred_layer_board)):\n",
        "  b = numpy_to_board(pred_layer_board[i].numpy())\n",
        "  if chess.STATUS_EMPTY in b.status():\n",
        "    cas_b_empty += 1\n",
        "    if first_i_not_empty == 0 and i > 1:\n",
        "      first_i_not_empty = i-1\n",
        "  else:\n",
        "    last_i_not_empty = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i81Ze83IQWzz"
      },
      "outputs": [],
      "source": [
        "b = numpy_to_board(pred_layer_board[first_i_not_empty].numpy())\n",
        "print(b.status())\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9GtQpwou7Uw"
      },
      "source": [
        "#Résultats générations configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL-955Uy9apY"
      },
      "outputs": [],
      "source": [
        "dataset_config.get_stats() #sur dataset apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etOIgjAd_giy"
      },
      "outputs": [],
      "source": [
        "print(f\"cas a : taux d'échec (validité) de {cas_a_invalide*100/len(reconstruction_layer_board)}%\")\n",
        "print(f\"cas a : taux de différence {cas_a_different*100/len(reconstruction_layer_board)}%\")\n",
        "print(f\"cas a : [DT]taux d'échec (validité) de {cas_a_invalide_DT*100/len(reconstruction_layer_board)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRYvSR7AyBoP"
      },
      "outputs": [],
      "source": [
        "print(f\"cas b : taux d'échec de {cas_b_invalide*100/len(pred_layer_board)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UxL9y2LKzCO"
      },
      "source": [
        "#Environnement Gym pour les échecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRg_7yLZwpe"
      },
      "source": [
        "##Librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogFf6bhdLOcr"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvRbk4tiZ65-"
      },
      "source": [
        "##Class ChessForRigEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0yc1bhPaLfV"
      },
      "outputs": [],
      "source": [
        "class ChessForRigEnv(gym.Env):\n",
        "      \"\"\"Environnement de jeu d'échec au format OpenAI gym\"\"\"\n",
        "      metadata = {'render.modes': ['human']}\n",
        "\n",
        "      def __init__(self):\n",
        "        super(ChessForRigEnv, self).__init__()\n",
        "\n",
        "        self.board = chess.Board()\n",
        "\n",
        "        # Action = 1 déplacement de pièce : move from, move to\n",
        "        # Cela revient à une position de départ dans l'échiquier (64 cases) : from\n",
        "        # vers une autre position : to\n",
        "        # donc c'est un tableau de 64x64 = 4096 positions \n",
        "        self.action_space = spaces.Discrete(4096) #spaces.Box(0, 4096, shape=(1,), dtype=np.float32) #spaces.Discrete(4096)\n",
        "\n",
        "        # Observation = configuration échiquier\n",
        "        # On reprend la représentation numérique de l'échiquier (6,8,8)\n",
        "        # On met la structure à plat par contre\n",
        "        self.observation_space = spaces.Box(-1, 1, shape=(6*8*8,), dtype=int)\n",
        "\n",
        "      def reset(self,seed=None, return_info=False, options=None):\n",
        "        self.board = chess.Board()\n",
        "        return np.reshape(board_to_numpy(self.board),(6*8*8,))\n",
        "\n",
        "      def step(self, action, mode_abandon=False):\n",
        "          is_null_move = False\n",
        "          move_from = int(action) // 64\n",
        "          move_to = int(action) % 64\n",
        "          moves = [x for x in self.board.generate_legal_moves() if \\\n",
        "                   x.from_square == move_from and x.to_square == move_to]\n",
        "          if len(moves) > 1:\n",
        "            move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "          elif len(moves) == 1:\n",
        "            move = moves[0]\n",
        "          else:\n",
        "            move = chess.Move.null() # le joueur ne sait pas quoi jouer !\n",
        "            is_null_move = True\n",
        "            #print(\"null move\")\n",
        "\n",
        "          if not is_null_move:\n",
        "            piece_balance_before = self.get_material_value()\n",
        "            self.board.push(move) # le joueur joue (notre agent)\n",
        "            piece_balance_after = self.get_material_value()\n",
        "\n",
        "            if not self.board.is_game_over():\n",
        "              opposant_move = get_random_action(self.board)\n",
        "              self.board.push(opposant_move) # l'opposant joue\n",
        "              reward = self.calcul_rewards(piece_balance_after, piece_balance_before)\n",
        "            else:\n",
        "              reward = self.calcul_rewards(0, 0)\n",
        "\n",
        "            done = self.board.is_game_over()\n",
        "            obs = np.reshape(board_to_numpy(self.board),(6*8*8,))\n",
        "          else:\n",
        "            #opposant_move = get_random_action(self.board)\n",
        "            #self.board.push(opposant_move) # l'opposant joue à ma place\n",
        "            #if not self.board.is_game_over():\n",
        "            #  opposant_move = get_random_action(self.board)\n",
        "            #  self.board.push(opposant_move) # l'opposant joue son tour             \n",
        "            if mode_abandon == True:\n",
        "              done = True\n",
        "              reward = 0\n",
        "            else:\n",
        "              self.board.push(chess.Move.null()) # on passe notre tour\n",
        "              opposant_move = get_random_action(self.board)\n",
        "              self.board.push(opposant_move) # l'opposant joue\n",
        "              done = self.board.is_game_over()\n",
        "              reward = 0 #on ne récompense pas notre agent !\n",
        "\n",
        "            obs = np.reshape(board_to_numpy(self.board),(6*8*8,))\n",
        "\n",
        "          return obs, reward, done, {}\n",
        "      \n",
        "      def render(self, mode='human',display=False):\n",
        "        if display:\n",
        "          print(self.board)\n",
        "        return self.board\n",
        "\n",
        "      def get_material_value(self):\n",
        "        \"\"\"\n",
        "        Sums up the material balance using Reinfield values\n",
        "        Returns: The material balance on the board\n",
        "        \"\"\"\n",
        "        layer_board = board_to_numpy(self.board)\n",
        "\n",
        "        pawns = 1 * np.sum(layer_board[0, :, :])\n",
        "        rooks = 5 * np.sum(layer_board[1, :, :])\n",
        "        minor = 3 * np.sum(layer_board[2:4, :, :])\n",
        "        queen = 9 * np.sum(layer_board[4, :, :])\n",
        "\n",
        "        return pawns + rooks + minor + queen\n",
        "\n",
        "      def calcul_rewards(self, piece_balance_after, piece_balance_before):\n",
        "\n",
        "        capture_reward = piece_balance_after - piece_balance_before\n",
        "        reward = capture_reward\n",
        "\n",
        "        if self.board.is_game_over():\n",
        "          if self.board.result() == \"*\":\n",
        "            reward = 0 + reward\n",
        "          else:\n",
        "            if self.board.result() == \"1-0\":\n",
        "              reward = 10 + reward\n",
        "            else:\n",
        "              reward = -1 + reward\n",
        "\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoDn3U548DuG"
      },
      "outputs": [],
      "source": [
        "action_space = spaces.Box(0, 4096, shape=(1,), dtype=np.float64)\n",
        "int(action_space.sample())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSvgalPb9ck2"
      },
      "outputs": [],
      "source": [
        "np.array([1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp0tj-DM83NK"
      },
      "outputs": [],
      "source": [
        "action_space = spaces.Discrete(4096)\n",
        "action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hS5o0JlKVGF"
      },
      "outputs": [],
      "source": [
        "obs_space = spaces.Box(-1, 1, shape=(6,8,8,), dtype=int)\n",
        "t = obs_space.sample()\n",
        "np.reshape(t,(6*8*8,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE-uIgIqWD_E"
      },
      "source": [
        "##Test ChessForRigEnv (test avec stockfish)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5wtgBmGWCCY"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "rounds = 2\n",
        "max_iter_per_round = 1000\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "env = ChessForRigEnv()\n",
        "\n",
        "for i in range(rounds):\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  cumulative_rewards = 0\n",
        "  k = 0\n",
        "  \n",
        "  while not done and k < max_iter_per_round:\n",
        "    result = engine.play(env.board, chess.engine.Limit(time=0.1, depth=4, nodes=3))\n",
        "    result = result.move\n",
        "    #result = get_random_action(env.board)\n",
        "    obs, reward, done, info = env.step(result.from_square*64+result.to_square) #env.step(np.array([result.from_square*64+result.to_square]))\n",
        "    #print(env.board.status())\n",
        "    cumulative_rewards = reward + cumulative_rewards\n",
        "    k += 1\n",
        "\n",
        "b = env.render(display=False)\n",
        "print(k, cumulative_rewards, b.result(), b.status())\n",
        "engine.quit()\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utilisation TD3, A2C, DQN stable baselines avec ChessForRigEnv"
      ],
      "metadata": {
        "id": "LATnV53Uf3S0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvytjbhf5Vel"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import TD3, A2C, DQN\n",
        "#from stable_baselines3.td3.policies import MlpPolicy, CnnPolicy, MultiInputPolicy\n",
        "from stable_baselines3.a2c.policies import MlpPolicy, CnnPolicy, MultiInputPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "\n",
        "#torch.set_default_dtype(torch.float32)\n",
        "\n",
        "env = ChessForRigEnv()\n",
        "\n",
        "# The noise objects for TD3\n",
        "#n_actions = env.action_space.shape[-1]\n",
        "#action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "#model = TD3(MlpPolicy, env, action_noise=action_noise, verbose=1)\n",
        "#model = A2C(\"MlpPolicy\", env, verbose=0)\n",
        "model = DQN(\"MlpPolicy\", env, verbose=1, batch_size=128, target_update_interval=1000, \n",
        "            exploration_initial_eps=0.5, exploration_final_eps=0.02, learning_rate=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=5000000, log_interval=1000)\n",
        "model.save(\"dqn_chess\")"
      ],
      "metadata": {
        "id": "8PhShUqdhJRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9nzDKFlNovS"
      },
      "outputs": [],
      "source": [
        "#del model # remove to demonstrate saving and loading\n",
        "#model.save(\"dqn_chess\")\n",
        "#model = TD3.load(\"td3_chess\")\n",
        "\n",
        "env = ChessForRigEnv()\n",
        "\n",
        "obs = env.reset()\n",
        "cumulative_rewards = 0\n",
        "done = False\n",
        "i = 0\n",
        "\n",
        "while not done and i < 300:\n",
        "    action, _ = model.predict(obs)\n",
        "    move = action #np.array([action])\n",
        "    obs, rewards, done, info = env.step(move, mode_abandon=False)\n",
        "    cumulative_rewards += rewards\n",
        "    i += 1\n",
        "\n",
        "b = env.render(display=False)\n",
        "print(i, action, cumulative_rewards, b.result(), b.status())\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntDiA-IjHxkd"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common import env_checker\n",
        "env = ChessForRigEnv()\n",
        "env_checker.check_env(env, warn=True, skip_render_check=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSh6I4RhJLP3"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "#obs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation DQN classique pour Chess Env\n",
        "(reprise du code RCP211/RL/TP3)"
      ],
      "metadata": {
        "id": "53lzw_YYzsm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import des librairies\n",
        "import os\n",
        "\n",
        "import random\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "#from IPython.display import HTML\n",
        "#from IPython import display as ipythondisplay\n",
        "#from pyvirtualdisplay import Display\n",
        "#display = Display(visible=0, size=(1400, 900))\n",
        "#display.start()\n",
        "\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import pylab as pl\n",
        "#from IPython import display as ipdisplay\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WkgSRO96z662"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Class DQNAgent \n",
        "\n",
        "class DQNAgent(object):\n",
        "    def __init__(self, env, model, replay1d, update_target_model ,ddqn=False,Soft_Update=False, dueling=False,batch_size=128,gamma=0.95,memory_size=1e4):\n",
        "              \n",
        "        self.env = env\n",
        "        self.env.seed(0)  \n",
        "        self.replay1d = replay1d\n",
        "        self.update_target_model = update_target_model\n",
        "\n",
        "        self.env._max_episode_steps = 4000\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES = 2000\n",
        "        \n",
        "        # Instantiate memory\n",
        "        memory_size = 10000\n",
        "        self.memory = deque(maxlen=65536)\n",
        "\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        \n",
        "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
        "        self.epsilon = 1.0  # exploration probability at start\n",
        "        self.epsilon_min = 0.01  # minimum exploration probability \n",
        "        self.epsilon_decay = 0.0005  # exponential decay rate for exploration prob\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # defining model parameters\n",
        "        self.ddqn = ddqn # use doudle deep q network\n",
        "        self.Soft_Update = Soft_Update # use soft parameter update\n",
        "        self.dueling = dueling\n",
        "        self.epsilon_greedy = False # use epsilon greedy improved strategy\n",
        "        \n",
        "        self.TAU = .5 # target network soft update hyperparameter\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        self.Model_name = os.path.join(self.Save_Path, \"ChessForRig_DQN.h5\")\n",
        "        \n",
        "        # create main model and target model\n",
        "        self.model = model(in_channels=self.state_size,action_space = self.action_size, dueling = False, name = 'online model').to(device)\n",
        "        self.target_model = model(in_channels=self.state_size,action_space = self.action_size, dueling = False, name = 'target model').to(device) \n",
        "\n",
        "        self.optimizer = optim.RMSprop(self.model.parameters(),lr=0.00025, eps=0.01, alpha=0.95)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.loss = 0\n",
        "        self.dd = torch.zeros(64, 4, 5, 5).to(device)\n",
        "        \n",
        "    # after some time interval update the target model to be the same as the online model               \n",
        "                \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        experience = state, action, reward, next_state, done\n",
        "        #print(f' state = {state}, action = {action}, reward = {reward}, next_state = {next_state}, done = {done}  ')\n",
        "        self.memory.append((experience))\n",
        "    \n",
        "    def act1d(self, state, decay_step):\n",
        "        # EPSILON GREEDY STRATEGY\n",
        "        if self.epsilon_greedy:\n",
        "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
        "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
        "        # OLD EPSILON STRATEGY\n",
        "        else:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= (1-self.epsilon_decay)\n",
        "            explore_probability = self.epsilon\n",
        "    \n",
        "        if explore_probability > np.random.rand():\n",
        "            # Make a random action (exploration)\n",
        "            #q = random.randrange(self.action_size)\n",
        "            result = get_random_action(self.env.board)\n",
        "            action = result.from_square*64+result.to_square\n",
        "            return action, explore_probability#int(np.argmax(q)), explore_probability\n",
        "\n",
        "        else:\n",
        "            # Get action from Q-network (exploitation)\n",
        "            # Estimate the Qs values state\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            q = self.model(torch.from_numpy(state).float().unsqueeze(0).to(device))\n",
        "            return int(torch.argmax(q)), explore_probability\n",
        "        \n",
        "    #pylab.figure(figsize=(18, 9))\n",
        "    def PlotModel(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
        "        pylab.plot(self.episodes, self.average, 'r')\n",
        "        pylab.plot(self.episodes, self.scores, 'b')\n",
        "        pylab.ylabel('Score', fontsize=18)\n",
        "        pylab.xlabel('Steps', fontsize=18)\n",
        "        dqn = 'DQN_'\n",
        "        softupdate = ''\n",
        "        dueling = ''\n",
        "        greedy = ''\n",
        "        PER = ''\n",
        "        if self.ddqn: dqn = 'DDQN_'\n",
        "        if self.Soft_Update: softupdate = '_soft'\n",
        "        if self.dueling: dueling = '_Dueling'\n",
        "        if self.epsilon_greedy: greedy = '_Greedy'\n",
        "        try:\n",
        "            pylab.savefig(dqn+\"ChessForRig\"+softupdate+dueling+greedy+PER+\"_CNN.png\")\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "        return str(self.average[-1])[:5]\n",
        "\n",
        "    \n",
        "    def reset1d(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    \n",
        "    def step1d(self,action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        return next_state, reward, done, info\n",
        "    \n",
        "    \n",
        "    def run1d(self):\n",
        "        decay_step = 0\n",
        "        loss_tab = []\n",
        "        dd = 0\n",
        "        dd_old = 0\n",
        "        \n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.reset1d()\n",
        "            done = False\n",
        "            i = 0\n",
        "            cum_rewards = 0\n",
        "            while not done:\n",
        "                decay_step += 1\n",
        "                action, explore_probability = self.act1d(state, decay_step)\n",
        "                next_state, reward, done, _ = self.step1d(action)\n",
        "\n",
        "                if not done or i == self.env._max_episode_steps-1:\n",
        "                    reward = reward\n",
        "                else:\n",
        "                    reward = -1e2\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "                cum_rewards += reward\n",
        "                if done:\n",
        "                    if e % 10==0:\n",
        "                        update_target_model(self.Soft_Update,self.ddqn,self.model,self.target_model,self.TAU)\n",
        "                    average = self.PlotModel(cum_rewards, e)#self.PlotModel(i, e)\n",
        "                    loss_tab.append(self.loss)\n",
        "                    print(\"episode: {}/{}, cum_rewards: {}, e: {:.2}, average: {}\\n\".format(e, self.EPISODES, cum_rewards, explore_probability, average))\n",
        "                    if i == self.env._max_episode_steps:\n",
        "                        print(\"Saving trained model to\", self.Model_name)\n",
        "                        break\n",
        "                self.replay1d(self.env,self.memory,self.batch_size,self.model,self.target_model,self.ddqn,self.gamma,self.criterion,self.loss,self.optimizer)\n",
        "        plt.plot(loss_tab)"
      ],
      "metadata": {
        "id": "CBRoRRdY0I50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title class DQN1d pour fonction valeur\n",
        "class DQN1d(nn.Module):\n",
        "\n",
        "    def __init__(self, action_space, in_channels, dueling=False, name='NN'):\n",
        "        super(DQN1d, self).__init__()\n",
        "        self.name = name\n",
        "        self.dueling = dueling\n",
        "        self.action_space = action_space\n",
        "\n",
        "        self.head1 = nn.Linear(in_channels, 512)\n",
        "        self.head2 = nn.Linear(512, 256)\n",
        "        self.head3 = nn.Linear(256, 64)\n",
        "        \n",
        "        if dueling:\n",
        "            self.state_value = nn.Linear(64,1)\n",
        "            self.action_advantage = nn.Linear(64,action_space)\n",
        "        else:\n",
        "            self.Q = nn.Linear(64,action_space)\n",
        "            \n",
        "        \n",
        "    def forward(self, x):\n",
        "        #print(f'{self.name} input = {x.shape}')\n",
        "        x = F.relu(self.head1(x))\n",
        "        x = F.relu(self.head2(x))\n",
        "        x = F.relu(self.head3(x))\n",
        "        \n",
        "        if self.dueling:\n",
        "            s = self.state_value(x).repeat((1,self.action_space))\n",
        "            a = self.action_advantage(x)\n",
        "            a -= torch.mean(a,dim=1, keepdim=True).repeat(1,2)\n",
        "            x = s+a\n",
        "        else:\n",
        "            x = self.Q(x)       \n",
        "        \n",
        "        #print(f'{self.name} output = {x.size()}')\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C9x_RpOF0sH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title replay1d\n",
        "def replay1d(env,memory,batch_size,model,target_model,ddqn,gamma,criterion,loss,optimizer):\n",
        "        # Randomly sample minibatch from the deque memory\n",
        "        minibatch = random.sample(memory, min(len(memory), batch_size))\n",
        "\n",
        "        state = np.zeros([batch_size,env.observation_space.shape[0]])\n",
        "        #print(state.shape)\n",
        "        next_state = np.zeros([batch_size,env.observation_space.shape[0]])\n",
        "        action, reward, done = [], [], []\n",
        "\n",
        "        # do this before prediction\n",
        "        # for speedup, this could be done on the tensor level\n",
        "        # but easier to understand using a loop       \n",
        "        for i in range(len(minibatch)):\n",
        "            #print(\"minibatch[i][0] = \",minibatch[i][0])\n",
        "            state[i] = minibatch[i][0]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            done.append(minibatch[i][4])\n",
        "        # do batch prediction to save speed\n",
        "        # predict Q-values for starting state using the main network\n",
        "        target = model(torch.from_numpy(state).float().to(device)).detach()\n",
        "        target_old = target.clone()\n",
        "        # predict best action in ending state using the main network\n",
        "        target_next = model(torch.from_numpy(next_state).float().to(device)).detach()\n",
        "        # predict Q-values for ending state using the target network\n",
        "        target_val = target_model(torch.from_numpy(next_state).float().to(device)).detach()\n",
        "        for i in range(len(minibatch)):\n",
        "            # correction on the Q value for the action used\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                # the key point of Double DQN\n",
        "                # selection of action is from model\n",
        "                # update is from target model\n",
        "                if ddqn: # Double - DQN\n",
        "                    # current Q Network selects the action\n",
        "                    # a'_max = argmax_a' Q(s', a')\n",
        "                    a =  int(torch.argmax(target_next[i]))# np.argmax(target_next[i])\n",
        "                    # target Q Network evaluates the action\n",
        "                    # Q_max = Q_target(s', a'_max)\n",
        "                    target[i][action[i]] = reward[i] + gamma * (target_val[i][a])\n",
        "                else: # Standard - DQN\n",
        "                    # DQN chooses the max Q value among next actions\n",
        "                    # selection and evaluation of action is on the target Q Network\n",
        "                    # Q_max = max_a' Q_target(s', a')\n",
        "                    target[i][action[i]] = reward[i] + gamma * target_next[i].max(0)[0].detach()\n",
        "\n",
        "            \n",
        "                \n",
        "        # Train the Neural Network with batches\n",
        "        # self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
        "        pred = model(torch.from_numpy(state).float().to(device))\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wk94ElrE1EoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_target_model\n",
        "def update_target_model(Soft_Update,ddqn,model,target_model,TAU):\n",
        "        if not Soft_Update:\n",
        "            \n",
        "            target_model.load_state_dict(model.state_dict()) \n",
        "            return\n",
        "        if Soft_Update and ddqn:    \n",
        "            with torch.no_grad():\n",
        "                #dict_model = dict(self.model.named_parameters())\n",
        "                #dict_target = dict(self.target.named_paramters())\n",
        "                for model, target in zip(model.named_parameters(), target_model.named_parameters()):\n",
        "                    model_name, model_weight = model\n",
        "                    target_name, target_weight = target\n",
        "                    #print(f'target_weight = {target_weight.data}')\n",
        "                    tmp = target_weight.data * (1-TAU) + model_weight.data * TAU\n",
        "                    target_weight.data.copy_(tmp)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xyRKMJPn1R4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run DQN classique dans l'environnement Chess"
      ],
      "metadata": {
        "id": "Cz70KifXZxvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = ChessForRigEnv()"
      ],
      "metadata": {
        "id": "FzsDfV8Q1eWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentDQN = DQNAgent(env, DQN1d, replay1d,update_target_model)\n",
        "agentDQN.run1d()"
      ],
      "metadata": {
        "id": "rRQnpYtq55-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agentDQN.target_model.state_dict(), \"target_model_dqn_RIG\")\n",
        "torch.save(agentDQN.model.state_dict(), \"model_dqn_RIG\")"
      ],
      "metadata": {
        "id": "WGCSHR0N7qS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_test = ChessForRigEnv()\n",
        "\n",
        "obs = env_test.reset()\n",
        "cumulative_rewards = 0\n",
        "done = False\n",
        "i = 0\n",
        "\n",
        "agentDQN.model.eval()\n",
        "\n",
        "while not done and i < 300:\n",
        "    q = agentDQN.model(torch.from_numpy(obs).float().unsqueeze(0).to(device))\n",
        "    move = int(torch.argmax(q))\n",
        "    obs, rewards, done, info = env_test.step(move, mode_abandon=False)\n",
        "    cumulative_rewards += rewards\n",
        "    i += 1\n",
        "\n",
        "b = env_test.render(display=False)\n",
        "print(i, move, cumulative_rewards, b.result(), b.status())\n",
        "b"
      ],
      "metadata": {
        "id": "cokke7zbWM_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentDQN.target_model.load_state_dict(torch.load(\"target_model_dqn_RIG\"))\n",
        "agentDQN.model.load_state_dict(torch.load(\"model_dqn_RIG\"))"
      ],
      "metadata": {
        "id": "OPGCy_i_-lJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-9ur7HDt5tHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation DQN RIG pour Chess Env\n",
        "(sur la base du code RCP211/RL/TP3)"
      ],
      "metadata": {
        "id": "IY_IFy2_5wQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import des librairies\n",
        "import os\n",
        "\n",
        "import random\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "#from IPython.display import HTML\n",
        "#from IPython import display as ipythondisplay\n",
        "#from pyvirtualdisplay import Display\n",
        "#display = Display(visible=0, size=(1400, 900))\n",
        "#display.start()\n",
        "\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import pylab as pl\n",
        "#from IPython import display as ipdisplay\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from numpy import linalg as LA\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6YzZUH2T5wQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Class DQN_RIG_Agent \n",
        "\n",
        "class DQN_RIG_Agent(object):\n",
        "    def __init__(self, env, vae, model, replay1d, update_target_model ,ddqn=False,Soft_Update=False, dueling=False,batch_size=128,gamma=0.95,memory_size=65536, episodes=2000):\n",
        "              \n",
        "        self.env = env\n",
        "        self.vae = vae\n",
        "        self.env.seed(0)  \n",
        "        self.replay1d = replay1d\n",
        "        self.update_target_model = update_target_model\n",
        "\n",
        "        self.env._max_episode_steps = 4000\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES = episodes\n",
        "        \n",
        "        # Instantiate memory\n",
        "        #memory_size = 65536\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "\n",
        "        self.gamma = gamma    # discount rate\n",
        "        \n",
        "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
        "        self.epsilon = 1.0  # exploration probability at start\n",
        "        self.epsilon_min = 0.01  # minimum exploration probability \n",
        "        self.epsilon_decay = 0.0005  # exponential decay rate for exploration prob\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # defining model parameters\n",
        "        self.ddqn = ddqn # use doudle deep q network\n",
        "        self.Soft_Update = Soft_Update # use soft parameter update\n",
        "        self.dueling = dueling\n",
        "        self.epsilon_greedy = False # use epsilon greedy improved strategy\n",
        "        \n",
        "        self.TAU = .1 # target network soft update hyperparameter\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        self.Model_name = os.path.join(self.Save_Path, \"ChessForRig_DQN.h5\")\n",
        "        \n",
        "        # create main model and target model\n",
        "        self.model = model(in_channels=self.state_size+latent_dimension,action_space = self.action_size, dueling = False, name = 'online model').to(device)\n",
        "        self.target_model = model(in_channels=self.state_size+latent_dimension,action_space = self.action_size, dueling = False, name = 'target model').to(device) \n",
        "\n",
        "        self.optimizer = optim.RMSprop(self.model.parameters(),lr=1e-3, eps=0.01, alpha=0.95)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.loss = 0\n",
        "        self.dd = torch.zeros(64, 4, 5, 5).to(device)\n",
        "        \n",
        "    # after some time interval update the target model to be the same as the online model               \n",
        "                \n",
        "    def remember(self, state, action, next_state, Zg, reward, done):\n",
        "        experience = state, action, next_state, Zg, reward, done\n",
        "        #print(f' state = {state}, action = {action}, reward = {reward}, next_state = {next_state}, done = {done}  ')\n",
        "        self.memory.append((experience))\n",
        "    \n",
        "    def act1d(self, state, decay_step, Z_g):\n",
        "        # EPSILON GREEDY STRATEGY\n",
        "        if self.epsilon_greedy:\n",
        "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
        "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
        "        # OLD EPSILON STRATEGY\n",
        "        else:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= (1-self.epsilon_decay)\n",
        "            explore_probability = self.epsilon\n",
        "    \n",
        "        if explore_probability > np.random.rand():\n",
        "            # Make a random action (exploration)\n",
        "            #q = random.randrange(self.action_size)\n",
        "            result = get_random_action(self.env.board)\n",
        "            action = result.from_square*64+result.to_square\n",
        "            return action, explore_probability#int(np.argmax(q)), explore_probability\n",
        "\n",
        "        else:\n",
        "            # Get action from Q-network (exploitation)\n",
        "            # Estimate the Qs values state\n",
        "            # Take the biggest Q value (= the best action)\n",
        "            q = self.model(torch.from_numpy(state).float().unsqueeze(0).to(device),\n",
        "                           torch.from_numpy(Z_g).float().to(device))\n",
        "            return int(torch.argmax(q)), explore_probability\n",
        "        \n",
        "    #pylab.figure(figsize=(18, 9))\n",
        "    def PlotModel(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
        "        pylab.plot(self.episodes, self.average, 'r')\n",
        "        pylab.plot(self.episodes, self.scores, 'b')\n",
        "        pylab.ylabel('Score', fontsize=18)\n",
        "        pylab.xlabel('Steps', fontsize=18)\n",
        "        dqn = 'DQN_'\n",
        "        softupdate = ''\n",
        "        dueling = ''\n",
        "        greedy = ''\n",
        "        PER = ''\n",
        "        if self.ddqn: dqn = 'DDQN_'\n",
        "        if self.Soft_Update: softupdate = '_soft'\n",
        "        if self.dueling: dueling = '_Dueling'\n",
        "        if self.epsilon_greedy: greedy = '_Greedy'\n",
        "        try:\n",
        "            pylab.savefig(dqn+\"ChessForRig\"+softupdate+dueling+greedy+PER+\"_CNN.png\")\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "        return str(self.average[-1])[:5]\n",
        "\n",
        "    \n",
        "    def reset1d(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    \n",
        "    def step1d(self,action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        return next_state, reward, done, info\n",
        "    \n",
        "    def convert_state_flat_to_cnn(self, state):\n",
        "        s = [np.reshape(state,(6,8,8))]\n",
        "        s = torch.tensor(s,dtype=torch.float32) \n",
        "        return s\n",
        "    \n",
        "    def run1d(self):\n",
        "        decay_step = 0\n",
        "        loss_tab = []\n",
        "        \n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.reset1d()\n",
        "            done = False\n",
        "            i = 0\n",
        "\n",
        "            cum_rewards = 0\n",
        "\n",
        "            Zg = torch.randn(1, latent_dimension, device='cpu')\n",
        "\n",
        "            while not done:\n",
        "                decay_step += 1\n",
        "                action, explore_probability = self.act1d(state, decay_step, Zg.numpy())\n",
        "                next_state, reward, done, _ = self.step1d(action)\n",
        "                \n",
        "                #le vae prend une structure matricielle\n",
        "                #state ou nex_state : (6*8*8,)\n",
        "                #conversion en (6,8,8)\n",
        "                #np.reshape(state,(6,8,8))\n",
        "                z, _ = self.vae.encoder(self.convert_state_flat_to_cnn(state))\n",
        "                z_prime, _ = self.vae.encoder(self.convert_state_flat_to_cnn(next_state))\n",
        "\n",
        "                if 0.5 > np.random.rand():\n",
        "                  Zg_prime = torch.randn(1, latent_dimension, device='cpu')\n",
        "                else:\n",
        "                  Zg_prime = Zg\n",
        "\n",
        "                #calcul reward\n",
        "                Z = z_prime-Zg_prime\n",
        "                \n",
        "                if not done or i == self.env._max_episode_steps-1:\n",
        "                    reward = reward\n",
        "                    r = -LA.norm(Z.detach().cpu())\n",
        "                else:\n",
        "                    reward = reward\n",
        "                    if reward == 0: #notre agent n'a pas remporté la partie\n",
        "                      r = -1e1\n",
        "                    else:\n",
        "                      r = 0\n",
        "                #state, action, next_state, Zg, reward (RIG), done    \n",
        "                self.remember(state, action, next_state, Zg_prime.detach().cpu(), r, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "                cum_rewards += reward\n",
        "\n",
        "                if done:\n",
        "                    if e % 10==0:\n",
        "                        update_target_model(self.Soft_Update,self.ddqn,self.model,self.target_model,self.TAU)\n",
        "                    average = self.PlotModel(cum_rewards, e)#self.PlotModel(i, e)\n",
        "                    loss_tab.append(self.loss)\n",
        "                    print(\"episode: {}/{}, e: {:.2}, average: {}\\n\".format(e, self.EPISODES, explore_probability, average))\n",
        "                    if i == self.env._max_episode_steps:\n",
        "                        print(\"Saving trained model to\", self.Model_name)\n",
        "                        break\n",
        "                self.replay1d(self.env,self.memory,self.batch_size,self.model,self.target_model,self.ddqn,self.gamma,self.criterion,self.loss,self.optimizer)\n",
        "        plt.plot(loss_tab)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6-BT0Tb35wQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title class DQN_RIG_1d pour fonction valeur\n",
        "class DQN_RIG_1d(nn.Module):\n",
        "\n",
        "    def __init__(self, action_space, in_channels, dueling=False, name='NN'):\n",
        "        super(DQN_RIG_1d, self).__init__()\n",
        "        self.name = name\n",
        "        self.dueling = dueling\n",
        "        self.action_space = action_space\n",
        "\n",
        "        self.head1 = nn.Linear(in_channels, 512)\n",
        "        self.head2 = nn.Linear(512, 256)\n",
        "        self.head3 = nn.Linear(256, 64)\n",
        "        \n",
        "        if dueling:\n",
        "            self.state_value = nn.Linear(64,1)\n",
        "            self.action_advantage = nn.Linear(64,action_space)\n",
        "        else:\n",
        "            self.Q = nn.Linear(64,action_space)\n",
        "            \n",
        "        \n",
        "    def forward(self, x, z_g):\n",
        "\n",
        "        #print(f'{self.name} x_size = {x.shape}, z_g_size = {z_g.shape}')\n",
        "\n",
        "        x = torch.concat((x, z_g), dim=1)\n",
        "        \n",
        "        x = F.relu(self.head1(x))\n",
        "        x = F.relu(self.head2(x))\n",
        "        x = F.relu(self.head3(x))\n",
        "        \n",
        "        if self.dueling:\n",
        "            s = self.state_value(x).repeat((1,self.action_space))\n",
        "            a = self.action_advantage(x)\n",
        "            a -= torch.mean(a,dim=1, keepdim=True).repeat(1,2)\n",
        "            x = s+a\n",
        "        else:\n",
        "            x = self.Q(x)       \n",
        "        \n",
        "        #print(f'{self.name} output = {x.size()}')\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GzxgLcbj5wQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title replay_RIG_1d\n",
        "def replay_RIG_1d(env,memory,batch_size,model,target_model,ddqn,gamma,criterion,loss,optimizer):\n",
        "        # Randomly sample minibatch from the deque memory\n",
        "        # memory : state, action, next_state, Zg, reward, done\n",
        "        #          0      1       2           3   4       5\n",
        "        minibatch = random.sample(memory, min(len(memory), batch_size))\n",
        "\n",
        "        state = np.zeros([batch_size,env.observation_space.shape[0]])\n",
        "        #print(state.shape)\n",
        "        next_state = np.zeros([batch_size,env.observation_space.shape[0]])\n",
        "        z_goals = np.zeros([batch_size,latent_dimension])\n",
        "        action, reward, done = [], [], []\n",
        "\n",
        "        # do this before prediction\n",
        "        # for speedup, this could be done on the tensor level\n",
        "        # but easier to understand using a loop       \n",
        "        for i in range(len(minibatch)):\n",
        "            #print(\"minibatch[i][0] = \",minibatch[i][0])\n",
        "            state[i] = minibatch[i][0]\n",
        "            z_goals[i] = minibatch[i][3]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][4])\n",
        "            next_state[i] = minibatch[i][2]\n",
        "            done.append(minibatch[i][5])\n",
        "        # do batch prediction to save speed\n",
        "        # predict Q-values for starting state using the main network\n",
        "        target = model(torch.from_numpy(state).float().to(device),torch.from_numpy(z_goals).float().to(device)).detach()\n",
        "        target_old = target.clone()\n",
        "        # predict best action in ending state using the main network\n",
        "        target_next = model(torch.from_numpy(next_state).float().to(device),torch.from_numpy(z_goals).float().to(device)).detach()\n",
        "        # predict Q-values for ending state using the target network\n",
        "        target_val = target_model(torch.from_numpy(next_state).float().to(device),torch.from_numpy(z_goals).float().to(device)).detach()\n",
        "        for i in range(len(minibatch)):\n",
        "            # correction on the Q value for the action used\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = torch.tensor(reward[i])\n",
        "            else:\n",
        "                # the key point of Double DQN\n",
        "                # selection of action is from model\n",
        "                # update is from target model\n",
        "                if ddqn: # Double - DQN\n",
        "                    # current Q Network selects the action\n",
        "                    # a'_max = argmax_a' Q(s', a')\n",
        "                    a =  int(torch.argmax(target_next[i]))# np.argmax(target_next[i])\n",
        "                    # target Q Network evaluates the action\n",
        "                    # Q_max = Q_target(s', a'_max)\n",
        "                    target[i][action[i]] = reward[i] + gamma * (target_val[i][a])\n",
        "                else: # Standard - DQN\n",
        "                    # DQN chooses the max Q value among next actions\n",
        "                    # selection and evaluation of action is on the target Q Network\n",
        "                    # Q_max = max_a' Q_target(s', a')\n",
        "                    target[i][action[i]] = reward[i] + gamma * target_next[i].max(0)[0].detach()\n",
        "\n",
        "            \n",
        "                \n",
        "        # Train the Neural Network with batches\n",
        "        # self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
        "        pred = model(torch.from_numpy(state).float().to(device),torch.from_numpy(z_goals).float().to(device))\n",
        "        loss = criterion(pred, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mlFKLtCM5wQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update_target_model_RIG\n",
        "def update_target_model_RIG(Soft_Update,ddqn,model,target_model,TAU):\n",
        "        if not Soft_Update:\n",
        "            \n",
        "            target_model.load_state_dict(model.state_dict()) \n",
        "            return\n",
        "        if Soft_Update and ddqn:    \n",
        "            with torch.no_grad():\n",
        "                #dict_model = dict(self.model.named_parameters())\n",
        "                #dict_target = dict(self.target.named_paramters())\n",
        "                for model, target in zip(model.named_parameters(), target_model.named_parameters()):\n",
        "                    model_name, model_weight = model\n",
        "                    target_name, target_weight = target\n",
        "                    #print(f'target_weight = {target_weight.data}')\n",
        "                    tmp = target_weight.data * (1-TAU) + model_weight.data * TAU\n",
        "                    target_weight.data.copy_(tmp)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Qz0LPdLw5wQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run DQN RIG dans l'environnement Chess"
      ],
      "metadata": {
        "id": "dI_PSHHF5wQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = ChessForRigEnv()"
      ],
      "metadata": {
        "id": "YdDfKVBZ5wQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentDQN_RIG = DQN_RIG_Agent(env, vae, DQN_RIG_1d, replay_RIG_1d,update_target_model_RIG, \n",
        "                             episodes=2000,gamma=0.99,ddqn=True,Soft_Update=True)"
      ],
      "metadata": {
        "id": "OMJWinspOpk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remplir la mémoire de l'expérience replay avec des observations de trajectoires de jeux gagnantes"
      ],
      "metadata": {
        "id": "US3mxf-ls3gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rounds = 500\n",
        "max_iter_per_round = 1000\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "env_fill_replay = ChessForRigEnv()\n",
        "\n",
        "nb_in_mem = 0\n",
        "\n",
        "for i in range(rounds):\n",
        "  state = env_fill_replay.reset()\n",
        "  done = False\n",
        "  k = 0\n",
        "  while not done and k < max_iter_per_round:\n",
        "    result = engine.play(env_fill_replay.board, chess.engine.Limit(time=0.1, depth=4, nodes=3))\n",
        "    result = result.move\n",
        "    action = result.from_square*64+result.to_square\n",
        "    next_state, reward, done, info = env_fill_replay.step(action)\n",
        "    \n",
        "    z, _ = agentDQN_RIG.vae.encoder(agentDQN_RIG.convert_state_flat_to_cnn(state))\n",
        "    z_prime, _ = agentDQN_RIG.vae.encoder(agentDQN_RIG.convert_state_flat_to_cnn(next_state))\n",
        "    Z = z-z_prime\n",
        "    r = -LA.norm(Z.detach().cpu())\n",
        "    agentDQN_RIG.remember(state, action, next_state, z_prime.detach().cpu(), r, done)\n",
        "    state = next_state\n",
        "    k += 1\n",
        "    nb_in_mem += 1\n",
        "\n",
        "engine.quit()"
      ],
      "metadata": {
        "id": "jpVDOZSKJneD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentDQN_RIG.model.train()\n",
        "agentDQN_RIG.run1d()"
      ],
      "metadata": {
        "id": "zW1D4X1V5wQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test en production du DQN RIG"
      ],
      "metadata": {
        "id": "P3Aezwkotapj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs_a_atteindre = np.reshape(board_to_numpy(board_gagnant),(6*8*8,))\n",
        "z_but, _ = agentDQN_RIG.vae.encoder(agentDQN_RIG.convert_state_flat_to_cnn(obs_a_atteindre))"
      ],
      "metadata": {
        "id": "ABIEHQj86F3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agentDQN.target_model.state_dict(), \"target_model_dqn_RIG\")\n",
        "torch.save(agentDQN.model.state_dict(), \"model_dqn_RIG\")"
      ],
      "metadata": {
        "id": "p6Wz1jXJ5wQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_test = ChessForRigEnv()\n",
        "\n",
        "obs = env_test.reset()\n",
        "cumulative_rewards = 0\n",
        "done = False\n",
        "i = 0\n",
        "\n",
        "agentDQN_RIG.model.eval()\n",
        "\n",
        "while not done and i < 300:\n",
        "    q = agentDQN_RIG.model(torch.from_numpy(obs).float().unsqueeze(0).to(device),\n",
        "                           z_but.to(device))\n",
        "    move = int(torch.argmax(q))\n",
        "    obs, rewards, done, info = env_test.step(move, mode_abandon=False)\n",
        "    cumulative_rewards += rewards\n",
        "    i += 1\n",
        "\n",
        "b = env_test.render(display=False)\n",
        "print(i, move, cumulative_rewards, b.result(), b.status())\n",
        "b"
      ],
      "metadata": {
        "id": "mvGfwHxH5wQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentDQN_RIG.target_model.load_state_dict(torch.load(\"target_model_dqn_RIG\"))\n",
        "agentDQN_RIG.model.load_state_dict(torch.load(\"model_dqn_RIG\"))"
      ],
      "metadata": {
        "id": "C7P3Celp5wQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tn-Jn_G6LSx"
      },
      "source": [
        "#Tests RLC \"capture\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A26QiMPtfWwz"
      },
      "source": [
        "##RLC \"capture\"\n",
        "[Reprise du code de la librairie RLC de arjangroen](https://github.com/arjangroen/RLC/tree/master/RLC/capture_chess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiX0CWLhfbNi"
      },
      "outputs": [],
      "source": [
        "#@title Agent\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras.models import Model, clone_model\n",
        "from keras.layers import Input, Conv2D, Dense, Reshape, Dot, Activation, Multiply\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def policy_gradient_loss(Returns):\n",
        "    def modified_crossentropy(action, action_probs):\n",
        "        cost = (K.categorical_crossentropy(action, action_probs, from_logits=False, axis=1) * Returns)\n",
        "        return K.mean(cost)\n",
        "\n",
        "    return modified_crossentropy\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, gamma=0.5, network='linear', lr=0.01, verbose=0):\n",
        "        \"\"\"\n",
        "        Agent that plays the white pieces in capture chess\n",
        "        Args:\n",
        "            gamma: float\n",
        "                Temporal discount factor\n",
        "            network: str\n",
        "                'linear' or 'conv'\n",
        "            lr: float\n",
        "                Learning rate, ideally around 0.1\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "        self.network = network\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose\n",
        "        self.init_network()\n",
        "        self.weight_memory = []\n",
        "        self.long_term_mean = []\n",
        "\n",
        "    def init_network(self):\n",
        "        \"\"\"\n",
        "        Initialize the network\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if self.network == 'linear':\n",
        "            self.init_linear_network()\n",
        "        elif self.network == 'conv':\n",
        "            self.init_conv_network()\n",
        "        elif self.network == 'conv_pg':\n",
        "            self.init_conv_pg()\n",
        "\n",
        "    def fix_model(self):\n",
        "        \"\"\"\n",
        "        The fixed model is the model used for bootstrapping\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        self.fixed_model = clone_model(self.model)\n",
        "        self.fixed_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "        self.fixed_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def init_linear_network(self):\n",
        "        \"\"\"\n",
        "        Initialize a linear neural network\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n",
        "        reshape_input = Reshape((512,))(input_layer)\n",
        "        output_layer = Dense(4096)(reshape_input)\n",
        "        self.model = Model(inputs=[input_layer], outputs=[output_layer])\n",
        "        self.model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    def init_conv_network(self):\n",
        "        \"\"\"\n",
        "        Initialize a convolutional neural network\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n",
        "        inter_layer_1 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        inter_layer_2 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        flat_1 = Reshape(target_shape=(1, 64))(inter_layer_1)\n",
        "        flat_2 = Reshape(target_shape=(1, 64))(inter_layer_2)\n",
        "        output_dot_layer = Dot(axes=1)([flat_1, flat_2])\n",
        "        output_layer = Reshape(target_shape=(4096,))(output_dot_layer)\n",
        "        self.model = Model(inputs=[input_layer], outputs=[output_layer])\n",
        "        self.model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    def init_conv_pg(self):\n",
        "        \"\"\"\n",
        "        Convnet net for policy gradients\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n",
        "        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n",
        "        R = Input(shape=(1,), name='Rewards')\n",
        "        legal_moves = Input(shape=(4096,), name='legal_move_mask')\n",
        "        inter_layer_1 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        inter_layer_2 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n",
        "        flat_1 = Reshape(target_shape=(1, 64))(inter_layer_1)\n",
        "        flat_2 = Reshape(target_shape=(1, 64))(inter_layer_2)\n",
        "        output_dot_layer = Dot(axes=1)([flat_1, flat_2])\n",
        "        output_layer = Reshape(target_shape=(4096,))(output_dot_layer)\n",
        "        softmax_layer = Activation('softmax')(output_layer)\n",
        "        legal_softmax_layer = Multiply()([legal_moves, softmax_layer])  # Select legal moves\n",
        "        self.model = Model(inputs=[input_layer, R, legal_moves], outputs=[legal_softmax_layer])\n",
        "        self.model.compile(optimizer=optimizer, loss=policy_gradient_loss(R))\n",
        "\n",
        "    def network_update(self, minibatch):\n",
        "        \"\"\"\n",
        "        Update the Q-network using samples from the minibatch\n",
        "        Args:\n",
        "            minibatch: list\n",
        "                The minibatch contains the states, moves, rewards and new states.\n",
        "\n",
        "        Returns:\n",
        "            td_errors: np.array\n",
        "                array of temporal difference errors\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Prepare separate lists\n",
        "        states, moves, rewards, new_states = [], [], [], []\n",
        "        td_errors = []\n",
        "        episode_ends = []\n",
        "        for sample in minibatch:\n",
        "            states.append(sample[0])\n",
        "            moves.append(sample[1])\n",
        "            rewards.append(sample[2])\n",
        "            new_states.append(sample[3])\n",
        "\n",
        "            # Episode end detection\n",
        "            if np.array_equal(sample[3], sample[3] * 0):\n",
        "                episode_ends.append(0)\n",
        "            else:\n",
        "                episode_ends.append(1)\n",
        "\n",
        "        # The Q target\n",
        "        q_target = np.array(rewards) + np.array(episode_ends) * self.gamma * np.max(\n",
        "            self.fixed_model.predict(np.stack(new_states, axis=0)), axis=1)\n",
        "\n",
        "        # The Q value for the remaining actions\n",
        "        q_state = self.model.predict(np.stack(states, axis=0))  # batch x 64 x 64\n",
        "\n",
        "        # Combine the Q target with the other Q values.\n",
        "        q_state = np.reshape(q_state, (len(minibatch), 64, 64))\n",
        "        for idx, move in enumerate(moves):\n",
        "            td_errors.append(q_state[idx, move[0], move[1]] - q_target[idx])\n",
        "            q_state[idx, move[0], move[1]] = q_target[idx]\n",
        "        q_state = np.reshape(q_state, (len(minibatch), 4096))\n",
        "\n",
        "        # Perform a step of minibatch Gradient Descent.\n",
        "        self.model.fit(x=np.stack(states, axis=0), y=q_state, epochs=1, verbose=0)\n",
        "\n",
        "        return td_errors\n",
        "\n",
        "    def get_action_values(self, state):\n",
        "        \"\"\"\n",
        "        Get action values of a state\n",
        "        Args:\n",
        "            state: np.ndarray with shape (8,8,8)\n",
        "                layer_board representation\n",
        "\n",
        "        Returns:\n",
        "            action values\n",
        "\n",
        "        \"\"\"\n",
        "        return self.fixed_model.predict(state) + np.random.randn() * 1e-9\n",
        "\n",
        "    def policy_gradient_update(self, states, actions, rewards, action_spaces, actor_critic=False):\n",
        "        \"\"\"\n",
        "        Update parameters with Monte Carlo Policy Gradient algorithm\n",
        "        Args:\n",
        "            states: (list of tuples) state sequence in episode\n",
        "            actions: action sequence in episode\n",
        "            rewards: rewards sequence in episode\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        n_steps = len(states)\n",
        "        Returns = []\n",
        "        targets = np.zeros((n_steps, 64, 64))\n",
        "        for t in range(n_steps):\n",
        "            action = actions[t]\n",
        "            targets[t, action[0], action[1]] = 1\n",
        "            if actor_critic:\n",
        "                R = rewards[t, action[0] * 64 + action[1]]\n",
        "            else:\n",
        "                R = np.sum([r * self.gamma ** i for i, r in enumerate(rewards[t:])])\n",
        "            Returns.append(R)\n",
        "\n",
        "        if not actor_critic:\n",
        "            mean_return = np.mean(Returns)\n",
        "            self.long_term_mean.append(mean_return)\n",
        "            train_returns = np.stack(Returns, axis=0) - np.mean(self.long_term_mean)\n",
        "        else:\n",
        "            train_returns = np.stack(Returns, axis=0)\n",
        "        # print(train_returns.shape)\n",
        "        targets = targets.reshape((n_steps, 4096))\n",
        "        self.weight_memory.append(self.model.get_weights())\n",
        "        self.model.fit(x=[np.stack(states, axis=0),\n",
        "                          train_returns,\n",
        "                          np.concatenate(action_spaces, axis=0)\n",
        "                          ],\n",
        "                       y=[np.stack(targets, axis=0)],\n",
        "                       verbose=self.verbose\n",
        "                       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs6WX8iyi91z"
      },
      "outputs": [],
      "source": [
        "#@title Environnement\n",
        "import chess\n",
        "import chess.engine\n",
        "import numpy as np\n",
        "\n",
        "mapper = {}\n",
        "mapper[\"p\"] = 0\n",
        "mapper[\"r\"] = 1\n",
        "mapper[\"n\"] = 2\n",
        "mapper[\"b\"] = 3\n",
        "mapper[\"q\"] = 4\n",
        "mapper[\"k\"] = 5\n",
        "mapper[\"P\"] = 0\n",
        "mapper[\"R\"] = 1\n",
        "mapper[\"N\"] = 2\n",
        "mapper[\"B\"] = 3\n",
        "mapper[\"Q\"] = 4\n",
        "mapper[\"K\"] = 5\n",
        "\n",
        "\n",
        "class Board(object):\n",
        "\n",
        "    def __init__(self, FEN=None, victory_reward=0):\n",
        "        \"\"\"\n",
        "        Chess Board Environment\n",
        "        Args:\n",
        "            FEN: str\n",
        "                Starting FEN notation, if None then start in the default chess position\n",
        "        \"\"\"\n",
        "        self.FEN = FEN\n",
        "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "        self.init_action_space()\n",
        "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        self.init_layer_board()\n",
        "        self.victory_reward = victory_reward\n",
        "\n",
        "    def init_action_space(self):\n",
        "        \"\"\"\n",
        "        Initialize the action space\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        self.action_space = np.zeros(shape=(64, 64))\n",
        "\n",
        "    def init_layer_board(self):\n",
        "        \"\"\"\n",
        "        Initalize the numerical representation of the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        self.layer_board = np.zeros(shape=(8, 8, 8))\n",
        "        for i in range(64):\n",
        "            row = i // 8\n",
        "            col = i % 8\n",
        "            piece = self.board.piece_at(i)\n",
        "            if piece == None:\n",
        "                continue\n",
        "            elif piece.symbol().isupper():\n",
        "                sign = 1\n",
        "            else:\n",
        "                sign = -1\n",
        "            layer = mapper[piece.symbol()]\n",
        "            self.layer_board[layer, row, col] = sign\n",
        "        if self.board.turn:\n",
        "            self.layer_board[6, :, :] = 1 / self.board.fullmove_number\n",
        "        if self.board.can_claim_draw():\n",
        "            self.layer_board[7, :, :] = 1\n",
        "\n",
        "    def step(self, action, opponent=None):\n",
        "        \"\"\"\n",
        "        Run a step\n",
        "        Args:\n",
        "            action: tuple of 2 integers\n",
        "                Move from, Move to\n",
        "        Returns:\n",
        "            epsiode end: Boolean\n",
        "                Whether the episode has ended\n",
        "            reward: int\n",
        "                Difference in material value after the move\n",
        "        \"\"\"\n",
        "        piece_balance_before = self.get_material_value()\n",
        "        self.board.push(action)\n",
        "        self.init_layer_board()\n",
        "        piece_balance_after = self.get_material_value()\n",
        "        if self.board.result() == \"*\":\n",
        "            if opponent == None:\n",
        "              opponent_move = self.get_random_action()\n",
        "            else:\n",
        "              PlayResult = opponent.play(self.board, chess.engine.Limit(time=0.1, depth=4, nodes=4))\n",
        "              opponent_move = PlayResult.move\n",
        "            self.board.push(opponent_move)\n",
        "            self.init_layer_board() \n",
        "            capture_reward = piece_balance_after - piece_balance_before\n",
        "            if self.board.result() == \"*\":\n",
        "                reward = 0 + capture_reward\n",
        "                episode_end = False\n",
        "            else:\n",
        "                if self.board.result() == \"1-0\":\n",
        "                  reward = self.victory_reward + capture_reward\n",
        "                else:\n",
        "                  reward = 0 + capture_reward\n",
        "                episode_end = True\n",
        "        else:\n",
        "            capture_reward = piece_balance_after - piece_balance_before\n",
        "            if self.board.result() == \"1-0\":\n",
        "                  reward = self.victory_reward + capture_reward\n",
        "            else:\n",
        "                  reward = 0 + capture_reward\n",
        "            episode_end = True\n",
        "        if self.board.is_game_over():\n",
        "            reward = 0\n",
        "            episode_end = True\n",
        "        return episode_end, reward\n",
        "\n",
        "    def get_random_action(self):\n",
        "        \"\"\"\n",
        "        Sample a random action\n",
        "        Returns: move\n",
        "            A legal python chess move.\n",
        "        \"\"\"\n",
        "        legal_moves = [x for x in self.board.generate_legal_moves()]\n",
        "        legal_moves = np.random.choice(legal_moves)\n",
        "        return legal_moves\n",
        "\n",
        "    def project_legal_moves(self):\n",
        "        \"\"\"\n",
        "        Create a mask of legal actions\n",
        "        Returns: np.ndarray with shape (64,64)\n",
        "        \"\"\"\n",
        "        self.action_space = np.zeros(shape=(64, 64))\n",
        "        moves = [[x.from_square, x.to_square] for x in self.board.generate_legal_moves()]\n",
        "        for move in moves:\n",
        "            self.action_space[move[0], move[1]] = 1\n",
        "        return self.action_space\n",
        "\n",
        "    def get_material_value(self):\n",
        "        \"\"\"\n",
        "        Sums up the material balance using Reinfield values\n",
        "        Returns: The material balance on the board\n",
        "        \"\"\"\n",
        "        pawns = 1 * np.sum(self.layer_board[0, :, :])\n",
        "        rooks = 5 * np.sum(self.layer_board[1, :, :])\n",
        "        minor = 3 * np.sum(self.layer_board[2:4, :, :])\n",
        "        queen = 9 * np.sum(self.layer_board[4, :, :])\n",
        "        return pawns + rooks + minor + queen\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "        self.init_layer_board()\n",
        "        self.init_action_space()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPfLCxW6jVMy"
      },
      "outputs": [],
      "source": [
        "#@title Learn\n",
        "import numpy as np\n",
        "from chess.pgn import Game\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Q_learning(object):\n",
        "\n",
        "    def __init__(self, agent, env, memsize=1000):\n",
        "        \"\"\"\n",
        "        Reinforce object to learn capture chess\n",
        "        Args:\n",
        "            agent: The agent playing the chess game as white\n",
        "            env: The environment including the python-chess board\n",
        "            memsize: maximum amount of games to retain in-memory\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.memory = []\n",
        "        self.memsize = memsize\n",
        "        self.reward_trace = []\n",
        "        self.memory = []\n",
        "        self.sampling_probs = []\n",
        "\n",
        "    def learn(self, iters=100, c=10, windows_r_average=10):\n",
        "        \"\"\"\n",
        "        Run the Q-learning algorithm. Play greedy on the final iter\n",
        "        Args:\n",
        "            iters: int\n",
        "                amount of games to train\n",
        "            c: int\n",
        "                update the network every c games\n",
        "\n",
        "        Returns: pgn (str)\n",
        "            pgn string describing final game\n",
        "\n",
        "        \"\"\"\n",
        "        for k in range(iters):\n",
        "            greedy = True if k == iters - 1 else False\n",
        "            self.env.reset()\n",
        "            self.play_game(k, greedy=greedy)\n",
        "            if k % c == 0:\n",
        "                all_r = np.array(self.reward_trace)\n",
        "                r_average = all_r[-windows_r_average:].mean()\n",
        "                print(\"iter\", k, \" r_average\", r_average)\n",
        "                self.agent.fix_model()\n",
        "\n",
        "        pgn = Game.from_board(self.env.board)\n",
        "        reward_smooth = pd.DataFrame(self.reward_trace)\n",
        "        reward_smooth.rolling(window=10, min_periods=0).mean().plot()\n",
        "\n",
        "        return pgn\n",
        "\n",
        "    def play_game(self, k, greedy=False, maxiter=25):\n",
        "        \"\"\"\n",
        "        Play a game of capture chess\n",
        "        Args:\n",
        "            k: int\n",
        "                game count, determines epsilon (exploration rate)\n",
        "            greedy: Boolean\n",
        "                if greedy, no exploration is done\n",
        "            maxiter: int\n",
        "                Maximum amount of steps per game\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "\n",
        "        # Here we determine the exploration rate. k is divided by 250 to slow down the exploration rate decay.\n",
        "        eps = max(0.05, 1 / (1 + (k / 250))) if not greedy else 0.\n",
        "\n",
        "        # Play a game of chess\n",
        "        while not episode_end:\n",
        "            state = self.env.layer_board\n",
        "            explore = np.random.uniform(0, 1) < eps  # determine whether to explore\n",
        "            if explore:\n",
        "                move = self.env.get_random_action()\n",
        "                move_from = move.from_square\n",
        "                move_to = move.to_square\n",
        "            else:\n",
        "                action_values = self.agent.get_action_values(np.expand_dims(state, axis=0))\n",
        "                action_values = np.reshape(np.squeeze(action_values), (64, 64))\n",
        "                action_space = self.env.project_legal_moves()  # The environment determines which moves are legal\n",
        "                action_values = np.multiply(action_values, action_space)\n",
        "                move_from = np.argmax(action_values, axis=None) // 64\n",
        "                move_to = np.argmax(action_values, axis=None) % 64\n",
        "                moves = [x for x in self.env.board.generate_legal_moves() if \\\n",
        "                         x.from_square == move_from and x.to_square == move_to]\n",
        "                if len(moves) == 0:  # If all legal moves have negative action value, explore.\n",
        "                    move = self.env.get_random_action()\n",
        "                    move_from = move.from_square\n",
        "                    move_to = move.to_square\n",
        "                else:\n",
        "                    move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "\n",
        "            episode_end, reward = self.env.step(move)\n",
        "            new_state = self.env.layer_board\n",
        "            if len(self.memory) > self.memsize:\n",
        "                self.memory.pop(0)\n",
        "                self.sampling_probs.pop(0)\n",
        "            turncount += 1\n",
        "            if turncount > maxiter:\n",
        "                episode_end = True\n",
        "                reward = 0\n",
        "            if episode_end:\n",
        "                new_state = new_state * 0\n",
        "            self.memory.append([state, (move_from, move_to), reward, new_state])\n",
        "            self.sampling_probs.append(1)\n",
        "\n",
        "            self.reward_trace.append(reward)\n",
        "\n",
        "            self.update_agent(turncount)\n",
        "\n",
        "        return self.env.board\n",
        "\n",
        "    def sample_memory(self, turncount):\n",
        "        \"\"\"\n",
        "        Get a sample from memory for experience replay\n",
        "        Args:\n",
        "            turncount: int\n",
        "                turncount limits the size of the minibatch\n",
        "\n",
        "        Returns: tuple\n",
        "            a mini-batch of experiences (list)\n",
        "            indices of chosen experiences\n",
        "\n",
        "        \"\"\"\n",
        "        minibatch = []\n",
        "        memory = self.memory[:-turncount]\n",
        "        probs = self.sampling_probs[:-turncount]\n",
        "        sample_probs = [probs[n] / np.sum(probs) for n in range(len(probs))]\n",
        "        indices = np.random.choice(range(len(memory)), min(1028, len(memory)), replace=True, p=sample_probs)\n",
        "        for i in indices:\n",
        "            minibatch.append(memory[i])\n",
        "\n",
        "        return minibatch, indices\n",
        "\n",
        "    def update_agent(self, turncount):\n",
        "        \"\"\"\n",
        "        Update the agent using experience replay. Set the sampling probs with the td error\n",
        "        Args:\n",
        "            turncount: int\n",
        "                Amount of turns played. Only sample the memory of there are sufficient samples\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if turncount < len(self.memory):\n",
        "            minibatch, indices = self.sample_memory(turncount)\n",
        "            td_errors = self.agent.network_update(minibatch)\n",
        "            for n, i in enumerate(indices):\n",
        "                self.sampling_probs[i] = np.abs(td_errors[n])\n",
        "\n",
        "\n",
        "class Reinforce(object):\n",
        "\n",
        "    def __init__(self, agent, env, opponent=None):\n",
        "        \"\"\"\n",
        "        Reinforce object to learn capture chess\n",
        "        Args:\n",
        "            agent: The agent playing the chess game as white\n",
        "            env: The environment including the python-chess board\n",
        "            memsize: maximum amount of games to retain in-memory\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.reward_trace = []\n",
        "        self.action_value_mem = []\n",
        "        self.opponent = opponent\n",
        "\n",
        "    def learn(self, iters=100, c=10):\n",
        "        \"\"\"\n",
        "        Run the Q-learning algorithm. Play greedy on the final iter\n",
        "        Args:\n",
        "            iters: int\n",
        "                amount of games to train\n",
        "            c: int\n",
        "                update the network every c games\n",
        "\n",
        "        Returns: pgn (str)\n",
        "            pgn string describing final game\n",
        "\n",
        "        \"\"\"\n",
        "        for k in range(iters):\n",
        "            self.env.reset()\n",
        "            states, actions, rewards, action_spaces = self.play_game(k)\n",
        "            self.reinforce_agent(states, actions, rewards, action_spaces)\n",
        "\n",
        "        pgn = Game.from_board(self.env.board)\n",
        "        reward_smooth = pd.DataFrame(self.reward_trace)\n",
        "        reward_smooth.rolling(window=10, min_periods=0).mean().plot()\n",
        "\n",
        "        return pgn\n",
        "\n",
        "    def play_game(self, k, maxiter=25):\n",
        "        \"\"\"\n",
        "        Play a game of capture chess\n",
        "        Args:\n",
        "            k: int\n",
        "                game count, determines epsilon (exploration rate)\n",
        "            greedy: Boolean\n",
        "                if greedy, no exploration is done\n",
        "            maxiter: int\n",
        "                Maximum amount of steps per game\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        action_spaces = []\n",
        "\n",
        "        # Play a game of chess\n",
        "        while not episode_end:\n",
        "            state = self.env.layer_board\n",
        "            action_space = self.env.project_legal_moves()  # The environment determines which moves are legal\n",
        "            action_probs = self.agent.model.predict([np.expand_dims(state, axis=0),\n",
        "                                                     np.zeros((1, 1)),\n",
        "                                                     action_space.reshape(1, 4096)])\n",
        "            self.action_value_mem.append(action_probs)\n",
        "            action_probs = action_probs / action_probs.sum()\n",
        "            move = np.random.choice(range(4096), p=np.squeeze(action_probs))\n",
        "            move_from = move // 64\n",
        "            move_to = move % 64\n",
        "            moves = [x for x in self.env.board.generate_legal_moves() if \\\n",
        "                     x.from_square == move_from and x.to_square == move_to]\n",
        "            assert len(moves) > 0  # should not be possible\n",
        "            if len(moves) > 1:\n",
        "                move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "            elif len(moves) == 1:\n",
        "                move = moves[0]\n",
        "\n",
        "            episode_end, reward = self.env.step(move, self.opponent)\n",
        "            new_state = self.env.layer_board\n",
        "            turncount += 1\n",
        "            if turncount > maxiter:\n",
        "                episode_end = True\n",
        "                reward = 0\n",
        "            if episode_end:\n",
        "                new_state = new_state * 0\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append((move_from, move_to))\n",
        "            rewards.append(reward)\n",
        "            action_spaces.append(action_space.reshape(1, 4096))\n",
        "\n",
        "        self.reward_trace.append(np.sum(rewards))\n",
        "\n",
        "        return states, actions, rewards, action_spaces\n",
        "\n",
        "    def reinforce_agent(self, states, actions, rewards, action_spaces):\n",
        "        \"\"\"\n",
        "        Update the agent using experience replay. Set the sampling probs with the td error\n",
        "        Args:\n",
        "            turncount: int\n",
        "                Amount of turns played. Only sample the memory of there are sufficient samples\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        self.agent.policy_gradient_update(states, actions, rewards, action_spaces)\n",
        "\n",
        "\n",
        "class ActorCritic(object):\n",
        "\n",
        "    def __init__(self, actor, critic, env):\n",
        "        \"\"\"\n",
        "        ActorCritic object to learn capture chess\n",
        "        Args:\n",
        "            actor: Policy Gradient Agent\n",
        "            critic: Q-learning Agent\n",
        "            env: The environment including the python-chess board\n",
        "            memsize: maximum amount of games to retain in-memory\n",
        "        \"\"\"\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.env = env\n",
        "        self.reward_trace = []\n",
        "        self.action_value_mem = []\n",
        "        self.memory = []\n",
        "        self.sampling_probs = []\n",
        "\n",
        "    def learn(self, iters=100, c=10):\n",
        "        \"\"\"\n",
        "        Run the Q-learning algorithm. Play greedy on the final iter\n",
        "        Args:\n",
        "            iters: int\n",
        "                amount of games to train\n",
        "            c: int\n",
        "                update the network every c games\n",
        "\n",
        "        Returns: pgn (str)\n",
        "            pgn string describing final game\n",
        "\n",
        "        \"\"\"\n",
        "        for k in range(iters):\n",
        "            if k % c == 0:\n",
        "                self.critic.fix_model()\n",
        "            self.env.reset()\n",
        "            end_state = self.play_game(k)\n",
        "\n",
        "        pgn = Game.from_board(self.env.board)\n",
        "        reward_smooth = pd.DataFrame(self.reward_trace)\n",
        "        reward_smooth.rolling(window=10, min_periods=0).mean().plot()\n",
        "\n",
        "        return pgn\n",
        "\n",
        "    def play_game(self, k, greedy=False, maxiter=25):\n",
        "        \"\"\"\n",
        "        Play a game of capture chess\n",
        "        Args:\n",
        "            k: int\n",
        "                game count, determines epsilon (exploration rate)\n",
        "            greedy: Boolean\n",
        "                if greedy, no exploration is done\n",
        "            maxiter: int\n",
        "                Maximum amount of steps per game\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "\n",
        "        # Play a game of chess\n",
        "        state = self.env.layer_board\n",
        "        while not episode_end:\n",
        "            state = self.env.layer_board\n",
        "            action_space = self.env.project_legal_moves()  # The environment determines which moves are legal\n",
        "            action_probs = self.actor.model.predict([np.expand_dims(state, axis=0),\n",
        "                                                     np.zeros((1, 1)),\n",
        "                                                     action_space.reshape(1, 4096)])\n",
        "            self.action_value_mem.append(action_probs)\n",
        "            # print(action_probs)\n",
        "            # print(np.max(action_probs))\n",
        "            action_probs = action_probs / action_probs.sum()\n",
        "            move = np.random.choice(range(4096), p=np.squeeze(action_probs))\n",
        "            move_from = move // 64\n",
        "            move_to = move % 64\n",
        "            moves = [x for x in self.env.board.generate_legal_moves() if \\\n",
        "                     x.from_square == move_from and x.to_square == move_to]\n",
        "            assert len(moves) > 0  # should not be possible\n",
        "            if len(moves) > 1:\n",
        "                move = np.random.choice(moves)  # If there are multiple max-moves, pick a random one.\n",
        "            elif len(moves) == 1:\n",
        "                move = moves[0]\n",
        "\n",
        "            episode_end, reward = self.env.step(move)\n",
        "            new_state = self.env.layer_board\n",
        "            turncount += 1\n",
        "            if turncount > maxiter:\n",
        "                episode_end = True\n",
        "                reward = 0\n",
        "            if episode_end:\n",
        "                new_state = new_state * 0\n",
        "\n",
        "            self.memory.append([state, (move_from, move_to), reward, new_state, action_space.reshape(1, 4096)])\n",
        "            self.sampling_probs.append(1)\n",
        "            self.reward_trace.append(reward)\n",
        "\n",
        "        self.update_actorcritic(turncount)\n",
        "\n",
        "        return self.env.board\n",
        "\n",
        "    def sample_memory(self, turncount):\n",
        "        \"\"\"\n",
        "        Get a sample from memory for experience replay\n",
        "        Args:\n",
        "            turncount: int\n",
        "                turncount limits the size of the minibatch\n",
        "\n",
        "        Returns: tuple\n",
        "            a mini-batch of experiences (list)\n",
        "            indices of chosen experiences\n",
        "\n",
        "        \"\"\"\n",
        "        minibatch = []\n",
        "        memory = self.memory[:-turncount]\n",
        "        probs = self.sampling_probs[:-turncount]\n",
        "        sample_probs = [probs[n] / np.sum(probs) for n in range(len(probs))]\n",
        "        indices = np.random.choice(range(len(memory)), min(1028, len(memory)), replace=False, p=sample_probs)\n",
        "        for i in indices:\n",
        "            minibatch.append(memory[i])\n",
        "\n",
        "        return minibatch, indices\n",
        "\n",
        "    def update_actorcritic(self, turncount):\n",
        "        \"\"\"Actor critic\"\"\"\n",
        "\n",
        "        if turncount < len(self.memory):\n",
        "\n",
        "            # Get a sampple\n",
        "            minibatch, indices = self.sample_memory(turncount)\n",
        "\n",
        "            # Update critic and find td errors for prioritized experience replay\n",
        "            td_errors = self.critic.network_update(minibatch)\n",
        "\n",
        "            # Get a Q value from the critic\n",
        "            states = [x[0] for x in minibatch]\n",
        "            actions = [x[1] for x in minibatch]\n",
        "            Q_est = self.critic.get_action_values(np.stack(states, axis=0))\n",
        "            action_spaces = [x[4] for x in minibatch]\n",
        "\n",
        "            self.actor.policy_gradient_update(states, actions, Q_est, action_spaces, actor_critic=True)\n",
        "\n",
        "            # Update sampling probs\n",
        "            for n, i in enumerate(indices):\n",
        "                self.sampling_probs[i] = np.abs(td_errors[n])\n",
        "\n",
        "    def update_critic(self, turncount):\n",
        "        \"\"\"\n",
        "        Update the agent using experience replay. Set the sampling probs with the td error\n",
        "        Args:\n",
        "            turncount: int\n",
        "                Amount of turns played. Only sample the memory of there are sufficient samples\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        if turncount < len(self.memory):\n",
        "            minibatch, indices = self.sample_memory(turncount)\n",
        "            td_errors = self.critic.network_update(minibatch)\n",
        "\n",
        "            for n, i in enumerate(indices):\n",
        "                self.sampling_probs[i] = np.abs(td_errors[n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLFASa3jB5C3"
      },
      "source": [
        "##Apprentissage Agents RL Reinforce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omKcb0ej6Qqb"
      },
      "outputs": [],
      "source": [
        "import tensorflow._api.v2.compat.v1 as tf\n",
        "import time\n",
        "\n",
        "import chess.engine\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "board = Board(victory_reward=5)\n",
        "agent = Agent(network='conv_pg',lr=0.1)\n",
        "R = Reinforce(agent,board)#, opponent=engine)\n",
        "\n",
        "start = time.time()\n",
        "pgn = R.learn(iters=8000)\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "\n",
        "engine.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlrRMs5t02Yf"
      },
      "source": [
        "##Parties agent RLC (Reinforce) contre lui même"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZrKeCCD1IDV"
      },
      "outputs": [],
      "source": [
        "def run_games_2(rounds=1000):\n",
        "  round_success_w = 0\n",
        "  round_success_b = 0\n",
        "  for i in range(rounds):\n",
        "      max_iter = 1000\n",
        "      k = 0\n",
        "      board = chess.Board()\n",
        "      while not board.is_game_over() and k < max_iter:\n",
        "          k += 1\n",
        "          if(board.turn): # blanc, notre agent joue\n",
        "            white_move = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(white_move)\n",
        "          else: # black, c'est encore le même agent qui joue\n",
        "            result = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(result)\n",
        "      success_w = 1 if board.result()==\"1-0\" else 0\n",
        "      success_b = 1 if board.result()==\"0-1\" else 0\n",
        "      round_success_w += success_w\n",
        "      round_success_b += success_b\n",
        "  return round_success_w/rounds, round_success_b/rounds\n",
        "\n",
        "rounds = 1000\n",
        "\n",
        "round_success_w, round_success_b = run_games_2(rounds=rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tGf9HlJ13Xh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "style.use('seaborn-pastel')\n",
        "\n",
        "Class = [\"1\"]\n",
        "w = 0.6\n",
        "round_success_nul = 1 - (round_success_w + round_success_b)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "color = 'steelblue'\n",
        "plt.bar(Class, round_success_w, w, label='blanc gagnant', color = color,alpha=0.3)\n",
        "plt.bar(Class, round_success_nul, w, bottom=round_success_w, label='match nul', color = color, alpha=0.7)\n",
        "plt.bar(Class, round_success_b, w, bottom=round_success_nul+round_success_w, label='noir gagnant', color = color)\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Répartition des victoires\")\n",
        "plt.title(\"Résultats RLC Reinforce vs RLC Reinforce - 1000 parties\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(round_success_w,round_success_nul, round_success_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw749ho_-o83"
      },
      "source": [
        "##Parties agent RLC (Reinforce) contre agent aléatoire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gekx2y1l-qMI"
      },
      "outputs": [],
      "source": [
        "def run_games_3(rounds=1000):\n",
        "  round_success_w = 0\n",
        "  round_success_b = 0\n",
        "  for i in range(rounds):\n",
        "      max_iter = 1000\n",
        "      k = 0\n",
        "      board = chess.Board()\n",
        "      while not board.is_game_over() and k < max_iter:\n",
        "          k += 1\n",
        "          if(board.turn): # blanc, un agent aléatoire pur\n",
        "            white_move = get_random_action(board)\n",
        "            board.push(white_move)\n",
        "          else: # black, c'est notre agent (un petit désavantage)\n",
        "            result = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(result)\n",
        "      success_w = 1 if board.result()==\"1-0\" else 0\n",
        "      success_b = 1 if board.result()==\"0-1\" else 0\n",
        "      round_success_w += success_w\n",
        "      round_success_b += success_b\n",
        "  return round_success_w/rounds, round_success_b/rounds\n",
        "\n",
        "rounds = 1000\n",
        "\n",
        "round_success_w, round_success_b = run_games_3(rounds=rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN30vjR1-3Zu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "style.use('seaborn-pastel')\n",
        "\n",
        "Class = [\"1\"]\n",
        "w = 0.6\n",
        "round_success_nul = 1 - (round_success_w + round_success_b)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "color = 'steelblue'\n",
        "plt.bar(Class, round_success_w, w, label='blanc gagnant', color = color,alpha=0.3)\n",
        "plt.bar(Class, round_success_nul, w, bottom=round_success_w, label='match nul', color = color, alpha=0.7)\n",
        "plt.bar(Class, round_success_b, w, bottom=round_success_nul+round_success_w, label='noir gagnant', color = color)\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Répartition des victoires\")\n",
        "plt.title(\"Résultats RLC Reinforce vs aléatoire - 1000 parties\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(round_success_w,round_success_nul, round_success_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxhkIV-NCHlj"
      },
      "source": [
        "##Parties avec Agent RLC (Reinforce) en blanc et stockfish en noir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ambK5VL57cc"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "\n",
        "rounds = 1000\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "\n",
        "def run_games(rounds=1000, time=0.1, depth=2, nodes=3):\n",
        "  round_success_w = 0\n",
        "  round_success_b = 0\n",
        "  for i in range(rounds):\n",
        "      max_iter = 1000\n",
        "      k = 0\n",
        "      board = chess.Board()\n",
        "      while not board.is_game_over() and k < max_iter:\n",
        "          k += 1\n",
        "          if(board.turn): # blanc, notre agent joue\n",
        "            white_move = next_white_move(agent, board, debug=False, best_probs=True)\n",
        "            board.push(white_move)\n",
        "          else: # black, c'est stockfish qui joue\n",
        "            result = engine.play(board, chess.engine.Limit(time=time, depth=depth, nodes=nodes))\n",
        "            board.push(result.move)\n",
        "      success_w = 1 if board.result()==\"1-0\" else 0\n",
        "      success_b = 1 if board.result()==\"0-1\" else 0\n",
        "      round_success_w += success_w\n",
        "      round_success_b += success_b\n",
        "  return round_success_w/rounds, round_success_b/rounds\n",
        "\n",
        "n = 4\n",
        "round_success_w = np.zeros(n)\n",
        "round_success_b = np.zeros(n)\n",
        "\n",
        "for i in range(n):\n",
        "  round_success_w[i], round_success_b[i] = run_games(rounds=rounds, time=0.1, depth=4, nodes=i+1)\n",
        "\n",
        "engine.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we1gljzoyRvy"
      },
      "outputs": [],
      "source": [
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish-5-linux/Linux/stockfish_14053109_x64\")\n",
        "round_success_w_10_noeuds, round_success_b_10_noeuds = run_games(rounds=rounds, time=0.1, depth=4, nodes=10)\n",
        "engine.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88L9cbo1h8_z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "\n",
        "style.use('seaborn-pastel')\n",
        "\n",
        "Class = [\"1 noeud\", \"2 noeuds\", \"3 noeuds\", \"4 noeuds\"]\n",
        "w = 0.6\n",
        "round_success_nul = 1 - (round_success_w + round_success_b)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "color = 'steelblue'\n",
        "plt.bar(Class, round_success_w, w, label='blanc gagnant', color = color,alpha=0.3)\n",
        "plt.bar(Class, round_success_nul, w, bottom=round_success_w, label='match nul', color = color, alpha=0.7)\n",
        "plt.bar(Class, round_success_b, w, bottom=round_success_nul+round_success_w, label='noir gagnant', color = color)\n",
        "plt.xlabel(\"Limites stockfish - noeuds (profondeur commune = 4)\")\n",
        "plt.ylabel(\"Répartition des victoires\")\n",
        "plt.title(\"Résultats RLC Reinforce vs Stockfish (limité) - 1000 parties\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(round_success_w,round_success_nul, round_success_b)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9AmDigYcuZBQ",
        "BNOEUNOavTxy",
        "PNgGDN1vueVU",
        "B9GtQpwou7Uw",
        "kIRg_7yLZwpe",
        "LATnV53Uf3S0",
        "A26QiMPtfWwz"
      ],
      "name": "Projet - Génération de scénarios et renforcement - Denis Lemarchand",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNmTmruROcl2op4zTQ5gC29",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}